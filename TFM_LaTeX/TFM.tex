\documentclass[11pt, twoside]{article}
\usepackage[latin1]{inputenc}
\usepackage[spanish,es-tabla]{babel}
\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{fixltx2e}
\usepackage[scaled=0.86]{helvet}
\usepackage{hyperref}
\usepackage{amssymb}
\usepackage{colortbl}
\usepackage{array}
\usepackage{tabularray}
\usepackage{vcell}
\usepackage{slashbox}
\usepackage{graphicx}
\usepackage{dcolumn}
\usepackage{vmargin}
\usepackage{multirow, array} % para las tablas
\usepackage{float}
%\usepackage[table]{xcolor}
\usepackage[dvipsnames]{xcolor}
\usepackage{fancyhdr}
\usepackage{cite} % para contraer referencias
\parindent 0pt
\hypersetup{
    colorlinks,
    citecolor=black,
    filecolor=black,
    linkcolor=black,
    urlcolor=blue
}
\usepackage{listings}
\lstdefinestyle{Python}{
    language        = Python,
    basicstyle      = \ttfamily\footnotesize,
    keywordstyle    = \color{blue},
    commentstyle    = \color{gray},
    stringstyle     = \color{green},
    showstringspaces= false,
    breaklines      = true
}
\usepackage{titlesec}

\setcounter{secnumdepth}{4}

%\titleformat{\paragraph}
%{\normalfont\normalsize\bfseries}{\theparagraph}{1em}{}
%\titlespacing*{\paragraph}
%{0pt}{3.25ex plus 1ex minus .2ex}{1.5ex plus .2ex}

\newcommand{\subsubsubsection}[1]{\paragraph{#1}\mbox{}\\}
\setcounter{secnumdepth}{4}
\setcounter{tocdepth}{4}

%%%%%%%% Coloca una imagen como encabezado %%%%%%%%%%%%%%
\pagestyle{fancy}
\fancyhf{}
%\fancyhead[L]{\includegraphics[width=.2\textwidth]{Logo3}}
%\fancyhead[R]{\rightmark}
\lhead{\includegraphics[width=.2\textwidth]{Logo3}}
\rhead{\rightmark}

\renewcommand{\sectionmark}[1]{\markright{\textit{\arabic{section}.\ #1}}}
%\renewcommand{\headrulewidth}{0.5pt}
\newlength\FHoffset
\setlength\FHoffset{1cm}
\addtolength\headwidth{2\FHoffset}
\fancyheadoffset{\FHoffset}

\fancyfoot{}
%\fancyfoot[RO, LE]{\thepage}
%\fancyfoot[LO,CE]{From: K. Grant}
%\fancyfoot[CO,RE]{To: Dean A. Smith}
\lfoot[\thepage]{Trabajo Fin de Máster}
\rfoot[Máster en Big Data y Data Science $\mid$ Edición abril 2022]{\thepage}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\setpapersize{A4}
\setmargins{3cm}       % margen izquierdo
{0.6cm}                        % margen superior
{15cm}                      % anchura del texto
{23.7cm}                    % altura del texto
{9pt}                           % altura de los encabezados
{2cm}                           % espacio entre el texto y los encabezados
{0pt}                             % altura del pie de página
{1.5cm}                           % espacio entre el texto y el pie de página

%\renewcommand{\listtablename}{Índice de tablasggg} 
\begin{document}

%\pagestyle{fancy}
%\lhead{\includegraphics[width=.2\textwidth]{Logo3}}

\begin{titlepage}
\begin{center}
\vspace*{-0.5in}

\begin{figure}[H]
	\begin{center}
	\includegraphics[width=0.5\textwidth]{Logo_VIU}
	\end{center}
\end{figure}
\vspace*{0.6 in}
\begin{Large}
\textbf{ANÁLISIS PREDICTIVO DE CASOS DE COVID-19 BASADO EN CONDICIONES CLIMÁTICAS} \\
\end{Large}
\vspace*{2.51 in}


\begin{large}
DANILO PLAZAS IRREÑO\\
DNI: 1024538287
\end{large}
\vspace*{2.51 in}
\begin{large}

UNIVERSIDAD INTERNACIONAL DE VALENCIA\\
FACULTAD DE MAESTRÍAS\\
MÁSTER EN BIG DATA Y DATA SCIENCE \\
BOGOTÁ D.C.\\
2022\\
\end{large}
\end{center}

\end{titlepage}

\newpage
\begin{titlepage}
\begin{center}
\begin{figure}[H]
	\begin{center}
	\includegraphics[width=0.5\textwidth]{Logo_VIU}
	\end{center}
\end{figure}
\vspace*{0.6in}
\begin{Large}
\textbf{ANÁLISIS PREDICTIVO DE CASOS DE COVID-19 BASADO EN CONDICIONES CLIMÁTICAS} \\
\end{Large}
\vspace*{0.86 in}

\begin{large}
DANILO PLAZAS IRREÑO\\
viudanilo0221p@gmail.com\\
DNI: 1024538287\\
\end{large}
\vspace*{0.86 in}
Trabajo de grado para optar al titulo de:\\ 
Magister en Big Data y Data Science\\
\vspace*{0.86 in}
\begin{large}
DIRECTOR:\\
MSc. BENJAMÍN ARROQUIA CUADROS\\
Docente Universidad Internacional de Valencia\\
\end{large}
\vspace*{0.87 in}
\begin{large}
UNIVERSIDAD INTERNACIONAL DE VALENCIA\\
FACULTAD DE MAESTRÍAS\\
MÁSTER EN BIG DATA Y DATA SCIENCE \\
BOGOTÁ D.C.\\
2022\\
\end{large}
\end{center}
\end{titlepage}

%\newpage
%\begin{titlepage}
%\bigskip
%\vspace*{2.8 in}
%\begin{flushright}
%``Nunca consideres el estudio como una\\
%obligación sino como una oportunidad\\
%para penetrar en el bello y maravilloso\\
%mundo del saber''.
%\\
%\bigskip
%Albert Einstein
%\end{flushright}
%\end{titlepage}

%\newpage
%\begin{titlepage}
%\begin{center}
%\textbf{\begin{Large}
%AGRADECIMIENTOS
%\end{Large}}
%\end{center}
%\bigskip
%Agradecemos de manera especial a nuestros padres, hermanos y hermanas ya que ellos fueron el principal cimiento para nuestra formación profesional sentando nuestras bases de responsabilidad y deseos de superación.
%\\\\
%También de manera fundamental agradecemos sinceramente a nuestro director Juan Carlos Gómez Paredes por sus conocimientos, su manera de trabajar, su persistencia, su paciencia y su motivación.
%Así como también agradecemos a nuestro revisor el Dr. Gustavo Adolfo Puerto Leguizamon por su conocimiento y el apoyo durante el desarrollo de este trabajo.
%\end{titlepage}

\newpage
\begin{titlepage}
\renewcommand{\contentsname}{\centering TABLA DE CONTENIDO}
\tableofcontents
\thispagestyle{empty}
\end{titlepage}

\newpage
\begin{titlepage}
\renewcommand{\listfigurename}{\centering ÍNDICE DE FIGURAS}
\listoffigures
\end{titlepage}

\newpage
\begin{titlepage}
\renewcommand{\listtablename}{\centering ÍNDICE DE TABLAS} 
\listoftables
\end{titlepage}

\newpage
\pagenumbering{arabic}
\begin{center}
\section*{RESUMEN}
\addcontentsline{toc}{section}{\protect\numberline{}RESUMEN}
\end{center}
Tras la aparición del covid-19 a nivel mundial a comienzos del 2020, han surgido varios estudios para identificar los factores que influyen en la propagación del virus, en donde el contacto cercano con personas infectadas es el principal factor, esto se debe a temas sociales como la movilidad, eventos, recreación, deporte, entre otros. Desafortunadamente no existe muchos estudios acerca de los factores climáticos, para este proyecto de TFM se sigue como hipótesis que las condiciones climáticas influyen en menor medida en la propagación del virus Covid-19.  En este proyecto se seguirá una metodología KDD persiguiendo los pasos habituales para la obtención de conocimiento y entendimiento de los datos.
\\\\ 
Se lleva a cabo un análisis de las bases de datos disponibles, así como la preparación y compresión de los datos y limpieza de los mismos. Los datos utilizados para este TFM han sido recopilados del Centro Nacional de Epidemiología (CNE) para los casos de Covid-19 reportados, los datos climatológicos proporcionados por el portal datos abiertos AEMET, los datos de población proporcionados por el Instituto Nacional de Estadística (INE). Con estos datos se ha generado un único dataset para la realización de los modelos de machine learning (regresión lineal múltiple, RandomForest Regresión, series temporales y K-means) para predecir o encontrar patrones sobre la tasa de incidencia (normalización de los casos de Covid-19) por provincia con relación a los factores climáticos durante la pandemia.
\\\\
Los resultados obtenidos con los distintos modelos no fueron los óptimos basado en nuestras métricas definidas ($R^2$, MAE y RMSE), este resultado es el esperado, ya que las condiciones climáticas no son el factor principal en la propagación del virus, las variables no logran explicar nuestra variable a predecir. Sin embargo, el modelo de K-means evidencia una influencia de los factores climáticos sobre la tasa de incidencia, mediante una segmentación de los casos en 3 clusters, en donde los clúster con ciertas características climáticas presentara una tasa de incidencia diferente. Todos los códigos estarán compartidos públicamente en el \href{https://github.com/Danilo0221/TFM}{REPOSITORIO TFM} del autor.
\\\\
\\\\
\textbf{Palabras clave:} KDD, clúster, $R^2$, MAE y RMSE.



\newpage
\begin{center}
\section{INTRODUCCIÓN}
\end{center}
\bigskip
El COVID-19 es una enfermedad respiratoria causada por el virus SARS-CoV-2. Desde su aparición en Wuhan, China a finales de 2019, ha afectado a millones de personas en todo el mundo. Los gobiernos de todo el mundo han implementado diversas medidas para prevenir la propagación del virus y proteger la salud pública.
\\\\
Algunas de las medidas más comunes fueron: cierre de fronteras, distanciamiento social (como el cierre de escuelas, lugares de trabajo y eventos públicos), uso de mascarillas, pruebas y rastreo de contactos (para identificar a las personas infectadas y rastrear a aquellos con los que han tenido contacto cercano), cierre de empresas y restricciones de actividades no esenciales (para reducir la cantidad de personas que se congregan en lugares públicos), campañas de concientización y educación pública. Todas estás medidas ayudan a mitigar la propagación del virus y aunque la transmisión del virus se produce principalmente por contacto cercano con personas infectadas, se ha investigado sobre la posible influencia de las condiciones climáticas en la propagación del virus. 
\\\\
En general, se cree que el clima cálido y húmedo puede reducir la propagación del virus, ya que el calor y la humedad pueden debilitar la capacidad del virus para sobrevivir en el aire y en las superficies. Sin embargo, los expertos señalan que no hay suficiente evidencia científica para afirmar que las altas temperaturas y la humedad reducen significativamente la transmisión del virus.
Por otro lado, el invierno y el clima frío pueden aumentar la transmisión del virus, ya que las personas tienden a pasar más tiempo en espacios cerrados y con poca ventilación, lo que facilita la propagación del virus de persona a persona. [1][2]
\\\\
En este proyecto se desarrollará un estudio y análisis sobre el impacto de las condiciones climáticas en la propagación del virus covid-19 en España y determinar si existe algún factor relacionado con la transmisión.


\newpage
\begin{center}
\section{OBJETIVOS}
\end{center} 
\bigskip
\subsection{OBJETIVO GENERAL}
\bigskip
\begin{itemize}
\item Identificar las características principales que afectan e influyen el aumento de personas contagiadas del virus de COVID-19 en España.
\end{itemize}
\medskip
\subsection{OBJETIVOS ESPECÍFICOS}
\bigskip
\begin{itemize}
\item Extraer, transformar y obtener conocimiento de las diferentes fuentes de información o bases de datos de COVID-19 en España, centrándonos en características climáticas.
\item Crear, comparar y contrastar los diferentes modelos de predicción y/o clustering sobre el COVID-19.
\item Seleccionar el modelo que predice o explica lo más exacto posible la influencia de las características climáticas en los casos de virus de COVID-19.
\item Precisar los efectos de otro tipo de características o variables en los modelos utilizados y evaluar sus desempeños.

\end{itemize}


\newpage
\begin{center}
\section{MACHINE LEARNING Y ESTADO DEL ARTE}
\end{center} 
\bigskip
El Machine Learning es una técnica de Inteligencia Artificial que permite a los sistemas informáticos aprender de manera automática a partir de datos y experiencias previas, sin ser programados explícitamente para cada tarea. En lugar de seguir un conjunto fijo de instrucciones, los sistemas de Machine Learning pueden aprender a partir de datos, identificando patrones y tendencias, y utilizando esta información para realizar predicciones o tomar decisiones.\\\\
El aprendizaje automático se basa en la idea de que los sistemas informáticos pueden aprender de manera similar a como lo hacen los seres humanos, mediante la identificación de patrones y la adaptación a nuevas situaciones. En lugar de requerir que se programen todas las posibles situaciones y resultados, también nos permite que los sistemas aprendan a partir de datos históricos y experiencias previas, y así puedan tomar decisiones informadas y precisas en tiempo real.
Programar computadoras para aprender de la experiencia eventualmente debería eliminar la necesidad de gran parte de este esfuerzo de programación detallado. Conforme a la definición de ML de Tom M. Mitchell: ``Se dice que un programa de computadora aprende de la experiencia E con respeto a alguna clase de tareas T y medida de rendimiento P, si su rendimiento en tareas en T, medido por P, mejora con la experiencia E'' [3].
Existen varios tipos de técnicas de Machine Learning, incluyendo el aprendizaje supervisado, el aprendizaje no supervisado y el aprendizaje por refuerzo. En el aprendizaje supervisado, el sistema aprende a partir de datos etiquetados previamente, mientras que, en el aprendizaje no supervisado, el sistema busca patrones y similitudes en los datos sin etiquetar. En el aprendizaje por refuerzo, el sistema aprende a partir de la retroalimentación del entorno.\\\\
Se aplica en una variedad de campos, como la detección de fraudes, la clasificación de imágenes, el análisis de sentimientos y la predicción de ventas, entre otros. A medida que los datos se vuelven cada vez más importantes y abundantes, el Machine Learning se está convirtiendo en una herramienta esencial para empresas e investigadores que buscan automatizar tareas y mejorar la toma de decisiones.

\bigskip
\bigskip

\subsection{APRENDIZAJE SUPERVISADO}
\bigskip
El aprendizaje supervisado es una técnica de ML que se basa en el uso de datos etiquetados previamente para entrenar un modelo de predicción o clasificación. En el aprendizaje supervisado, el modelo se entrena utilizando un conjunto de datos de entrenamiento que contiene ejemplos de entrada y salida esperada. El objetivo del modelo es aprender una función que pueda predecir la salida correcta para nuevas entradas nunca antes vistas.\\\\
Por ejemplo, en la clasificación de correos electrónicos como spam o no spam, el modelo se entrena con una gran cantidad de correos electrónicos etiquetados previamente como spam o no spam. Utilizando esta información, el modelo aprende a identificar patrones en los correos electrónicos que le permiten clasificarlos correctamente. Una de las principales ventajas del aprendizaje supervisado es que puede proporcionar predicciones precisas y confiables en una variedad de tareas.
Sin embargo, el aprendizaje supervisado también tiene algunas limitaciones. En particular, requiere grandes cantidades de datos etiquetados, lo que puede ser costoso y laborioso en algunos casos. Además, el modelo puede ser susceptible al sobreajuste si se entrena con demasiados datos, lo que significa que se adapta demasiado a los datos de entrenamiento y no generaliza bien a nuevos datos nunca antes vistos. Este a su vez se divide en problemas de regresión y clasificación.


\subsubsection{REGRESIÓN}
\medskip
Los problemas de regresión en el aprendizaje supervisado son aquellos en los que se busca establecer una relación funcional entre una variable de entrada (también llamada variable independiente o predictor) y una variable de salida (también llamada variable dependiente o respuesta) que toma valores continuos en lugar de discretos. Es decir, se busca predecir un valor numérico continuo en lugar de una etiqueta o clase discreta.
El objetivo de la regresión es encontrar una función que mejor se ajuste a los datos observados, de manera que pueda ser utilizada para predecir el valor de la variable de salida para nuevas observaciones de la variable de entrada.\\

Sin embargo, los problemas de regresión pueden presentar algunos desafíos, como:
\begin{itemize}
\item La presencia de valores atípicos o datos faltantes, que pueden afectar negativamente el ajuste del modelo y la precisión de las predicciones.
\item La elección de la función de regresión adecuada y de los parámetros del modelo, que pueden depender de la distribución de los datos y del objetivo de la predicción.
\item La evaluación de la calidad del modelo, que puede requerir el uso de medidas de error y de rendimiento específicas para problemas de regresión.
\end{itemize}

Para superar estos desafíos, se pueden utilizar técnicas de preprocesamiento de datos, selección de características, validación cruzada y ajuste de hiperparámetros, entre otras.
Estos modelos pueden ser lineales o no lineales. Los modelos lineales, como la regresión lineal simple o múltiple, buscan establecer una relación lineal entre las variables de entrada y la variable de salida [4]. Por otro lado, los modelos no lineales, como la regresión polinómica, la regresión logística, regresión de árbol de decisión, random forest o redes neuronales, permiten establecer relaciones no lineales entre las variables.


\subsubsubsection{REGRESIÓN LINEAL MÚLTIPLE}

Un modelo de regresión lineal múltiple es una técnica estadística que se utiliza para predecir la variable de respuesta (o variable dependiente) en función de dos o más variables predictoras (o variables independientes). Es una extensión del modelo de regresión lineal simple, que solo utiliza una variable predictora. La ecuación para un modelo de regresión lineal múltiple se puede escribir como:
\begin{equation}
y = b_o + b_1x_1 + b_2x_2 + b_nx_n + e
\end{equation}
donde:
\\\\
$y:$ \hspace{1em} Variable de respuesta (o variable dependiente) que se quiere predecir.\\
$x_1, x_2, ..., x_n:$ \hspace{0.3em} Variables predictoras (variables independientes) que se utilizan para predecir y.\\
$b_0, b_1, b_2, ..., b_n:$ \hspace{0.52em} Coeficientes de regresión que representan la relación entre cada variable predictora y la variable de respuesta.\\
$e:$ \hspace{1em} Error residual o término de error, que representa la variación de $y$ que no se puede explicar por las variables predictoras.
\\\\
El objetivo de un modelo de regresión lineal múltiple es estimar los coeficientes de regresión $(b_0, b_1, b_2, ..., b_n)$ de tal manera que la suma de los errores residuales sea lo más pequeña posible. Esto se logra mediante el método de mínimos cuadrados, que minimiza la suma de los cuadrados de los errores residuales. Para estimar los coeficientes de regresión, se utilizan técnicas como la matriz de diseño, el cálculo de la matriz inversa y la solución de sistemas de ecuaciones lineales. Una vez que se han estimado los coeficientes de regresión, se puede utilizar el modelo para hacer predicciones sobre la variable de respuesta para nuevos valores de las variables predictoras.[10][11]

Al igual que en el modelo de regresión lineal simple, el modelo de regresión lineal múltiple asume que hay una relación lineal entre las variables predictoras y la variable de respuesta, si la relación no es lineal, puede ser necesario utilizar otro tipo de modelo, por tal motivo es importante evaluar la calidad del modelo mediante técnicas como la validación cruzada y la evaluación de métricas como: 
\\
\begin{itemize}
\item \textbf{MAE (Error Absoluto Medio)}
\\\\
Es una métrica que calcula la media de los errores absolutos de las predicciones del modelo en relación con los valores reales de la variable dependiente. Es una medida de la magnitud promedio del error en las predicciones del modelo en términos absolutos. Un MAE bajo indica que las predicciones del modelo tienen un pequeño error promedio en relación con los valores reales de la variable dependiente, lo que indica que el modelo tiene un buen ajuste a los datos, el MAE se calcula mediante:\\
\begin{equation}
MAE = \frac{1}{n}\sum_{i=1}^{n} |y_i - \hat{y_i}|
\end{equation}
\\
El MAE no considera la dirección del error, es decir, si el modelo está subestimando o sobrestimando los valores reales. Por lo tanto, el MAE debe interpretarse junto con otras métricas de evaluación del modelo, como el error cuadrático medio (MSE) y el coeficiente de determinación $(R^2)$, para obtener una imagen completa del rendimiento del modelo.

\item \textbf{RMSE (Raiz del Error Cuadratico Medio)}
\\\\
Se calcula como la raíz cuadrada del promedio de los errores cuadráticos de las predicciones del modelo en relación con los valores reales de la variable dependiente. El RMSE es similar al MAE, pero penaliza más fuertemente las predicciones que tienen un error mayor, el RMSE es particularmente útil cuando los errores de predicción son importantes y se desea minimizar el error promedio de predicción al cuadrado y igual que el MAE, el RMSE no considera la dirección del error, es decir, si el modelo está subestimando o sobrestimando los valores reales, el MAE se calcula mediante:\\
\begin{equation}
RMSE = \sqrt{\frac{1}{n}\sum_{i=1}^{n} (y_i - \hat{y_i})^2}
\end{equation}
\\
Sin embargo, el RMSE es más sensible a los valores atípicos que el MAE, lo que significa que puede ser más apropiado en situaciones en las que los valores atípicos tienen un gran impacto en la precisión del modelo.

\item \textbf{$R^2$ (Coeficiente de Determinación)}
\\\\
Indica la proporción de la varianza total de la variable dependiente $(y)$ que es explicada por las variables independientes $(x_1, x_2, ..., x_n)$ incluidas en el modelo. En otras palabras, $R^2$ es una medida de qué tan bien las variables predictoras explican la variabilidad en la variable dependiente, se calcula mediante:\\
\begin{equation}
R^2 = \dfrac{\sum\limits_{i=1}^{n} (\hat{y_i} - \bar{y})^2}{\sum\limits_{i=1}^{n} (y_i - \bar{y})^2}
\end{equation}
\\
El valor de $R^2$ varía entre 0 y 1, y se interpreta como sigue:\\\\
$R^2 = 0:$ el modelo no explica ninguna variabilidad en la variable dependiente. 
$R^2 = 1:$ el modelo explica toda la variabilidad en la variable dependiente.
\\\\
En la práctica, $R^2$ generalmente toma valores intermedios, lo que significa que el modelo explica parte, pero no toda la variabilidad en la variable dependiente.
Un valor alto de $R^2$ indica que el modelo ajusta bien los datos y que las variables predictoras son buenas para predecir la variable dependiente. Sin embargo, un valor alto de $R^2$ no necesariamente significa que el modelo sea bueno. Puede haber otras variables que no se hayan incluido en el modelo que también puedan explicar la variabilidad en la variable dependiente, es por esto importante evaluar el modelo en función de otras métricas.
\end{itemize}

\subsubsubsection{RANDOM FOREST}

El modelo de Random Forest es una extensión del modelo de Bagging (Bootstrap Aggregating) que utiliza múltiples árboles de decisión para mejorar la precisión de la predicción. Bagging es un enfoque de aprendizaje automático que implica la creación de múltiples muestras de entrenamiento a partir del conjunto de datos original, utilizando muestreo con reemplazo. Luego, se entrena un modelo separado para cada muestra y se promedian las predicciones de los modelos para obtener una predicción final, esto ayuda a reducir el sobreajuste y mejorar la precisión de la predicción.\\
Random Forest utiliza múltiples árboles de decisión para hacer predicciones de valores numéricos, la principal diferencia es que en Random Forest, en cada nodo de decisión se selecciona un subconjunto aleatorio de características para realizar la división en lugar de considerar todas las características disponibles, al seleccionar un subconjunto aleatorio de características, el modelo de Random Forest puede reducir la correlación entre los árboles y aumentar la diversidad de los árboles en el modelo, esto puede mejorar la precisión del modelo y reducir el sobreajuste. [12]
\\\\
En Random Forest la idea es que cada árbol de decisión tenga una idea ligeramente diferente de cómo se relacionan las características con la variable objetivo.
Para hacer una predicción se evalúa cada muestra en cada árbol de decisión en el modelo y se toma la media de las predicciones resultantes, se puede expresar matemáticamente como:
\begin{equation}
y = \dfrac{1}{N}\sum\limits_{i=1}^{n} y_i
\end{equation}

donde y es la predicción de la variable objetivo para una muestra dada, $y_i$ es la predicción de la variable objetivo para esa muestra en el i-ésimo árbol de decisión, y $N$ es el número total de árboles en el modelo. [13]\\
Las predicciones que se apartan demasiado de la media no son deseables, ya que pueden estar basadas en un árbol de decisión que tiene una idea atípica de cómo se relacionan las características con la variable objetivo. A continuación, se muestra el modelo de un random forest:

\begin{figure}[H]
	\begin{center}
	\includegraphics[width=0.7\textwidth]{random_forest}
		\textbf{\caption{{\small Modelo de funcionamiento random forest.}}}
	{\scriptsize Fuente: [cnvrg.io]}
	\end{center}
\end{figure}

El modelo de Random Forest para regresión tiene varias ventajas sobre otros modelos de regresión, incluyendo:
\begin{itemize}
\item Es capaz de manejar tanto características numéricas como categóricas.
\item Es resistente al sobreajuste, lo que significa que es menos probable que se sobreajuste en comparación con otros modelos de regresión.
\item Es capaz de manejar conjuntos de datos grandes y complejos.
\end{itemize}

Sin embargo, el modelo de Random Forest para regresión también tiene algunas limitaciones, incluyendo:
\begin{itemize}
\item Es más difícil de interpretar que algunos otros modelos de regresión.
\item Puede ser más computacionalmente costoso que algunos otros modelos de regresión.
\item No proporciona información sobre la forma en que se relacionan las características con la variable objetivo.
\end{itemize}
Las métricas de evaluación comunes para un modelo de Random Forest de regresión son similares a las utilizadas en otros modelos de regresión, como los descritos en el modelo de regresión lineal múltiple.

\subsubsection{CLASIFICACIÓN}
\medskip
Los modelos de clasificación binaria son utilizados cuando se desea predecir una variable de salida que puede tomar únicamente dos valores posibles, como ``sí'' o ``no'', ``verdadero'' o ``falso'', etc. Ejemplos de modelos de clasificación binaria incluyen la regresión logística y la máquina de vectores de soporte.
Por otro lado, los modelos de clasificación no binaria son utilizados cuando se desea predecir una variable de salida que puede tomar más de dos valores posibles, como la clasificación de imágenes en diferentes categorías o la predicción de resultados deportivos. Ejemplos de modelos de clasificación no binaria incluyen los árboles de decisión, los bosques aleatorios y las redes neuronales.\\\\
Aunque los modelos de clasificación binaria y no binaria utilizan diferentes técnicas y algoritmos, el proceso general de construcción de modelos es el mismo. Se trata de identificar las variables de entrada más importantes, elegir un modelo adecuado, ajustar sus parámetros y evaluar su rendimiento utilizando medidas de evaluación adecuadas. binaria como no binaria. La elección del modelo adecuado dependerá del tipo de datos y del objetivo de la predicción. [5]

\bigskip

\subsection{APRENDIZAJE NO SUPERVISADO}
\bigskip
El aprendizaje no supervisado es un tipo de aprendizaje automático en el que el algoritmo se entrena con datos no etiquetados, es decir, sin información previa sobre las categorías a las que pertenecen los datos. A diferencia del aprendizaje supervisado, donde los algoritmos aprenden a partir de datos etiquetados, en el aprendizaje no supervisado, los algoritmos buscan patrones y estructuras en los datos sin ninguna orientación sobre lo que se debe buscar.
Se utiliza para descubrir patrones ocultos y estructuras en los datos, como grupos de datos similares, tendencias en los datos y relaciones entre variables. Algunos de los algoritmos de aprendizaje no supervisado más comunes incluyen la agrupación (clustering), la reducción de dimensionalidad y la asociación. 

\subsubsection{CLUSTERING}
\medskip
El clustering, también conocido como agrupamiento, es una técnica de aprendizaje no supervisado en la que se agrupan datos similares en grupos o clústeres. El objetivo del clustering es dividir un conjunto de datos en grupos, donde los objetos en cada clúster son similares entre sí y diferentes de los objetos en otros clústeres. Esto se logra mediante el uso de medidas de similitud o distancia para medir la distancia entre objetos.
Existen varios algoritmos de clustering, cada uno con sus propias fortalezas y debilidades. El algoritmo K-means es uno de los algoritmos más populares y ampliamente utilizados en el clustering. Funciona dividiendo el conjunto de datos en K clústeres y asignando cada objeto al clúster más cercano. Luego, se recalcula el centroide de cada clúster y se repite el proceso hasta que se alcanza una solución óptima.\\

Otro algoritmo popular de clustering es el clustering jerárquico. Este algoritmo construye una jerarquía de clústeres mediante la combinación iterativa de clústeres en subgrupos más grandes. El resultado es un árbol jerárquico que representa la estructura de agrupamiento de los datos.
El clustering se utiliza en muchas aplicaciones, como la segmentación de clientes, la clasificación de imágenes y la agrupación de documentos. Por ejemplo, en la segmentación de clientes, el clustering se puede utilizar para agrupar a los clientes en función de sus patrones de compra o preferencias. En la clasificación de imágenes, el clustering se puede utilizar para agrupar imágenes similares para su posterior análisis o clasificación.\\

En la agrupación de documentos, el clustering se puede utilizar para agrupar documentos similares en temas específicos.Sin embargo, es importante tener en cuenta que el clustering no siempre es la mejor opción para todos los conjuntos de datos y situaciones. Algunas limitaciones del clustering incluyen la necesidad de definir el número de clústeres de antemano, la sensibilidad a los valores atípicos y la dificultad de evaluar los resultados. Es importante seleccionar el algoritmo y los parámetros adecuados para obtener resultados precisos y significativos. [6]

\subsubsubsection{K-MEANS}

El algoritmo k-means busca un número predeterminado de grupos dentro de un conjunto de datos multidimensional sin etiquetar. Logra esto utilizando una concepción simple de cómo se ve el agrupamiento óptimo:
\begin{itemize}
\item El ``centro del grupo'' es la media aritmética de todos los puntos que pertenecen al grupo.
\item Cada punto está más cerca de su propio centro de conglomerado que de otros centros de conglomerados.
\end{itemize}
Esas dos suposiciones son la base del modelo de k-medias que funciona mediante el algoritmo de maximización de expectativas (E - M), es un algoritmo poderoso que surge en una variedad de contextos dentro de la ciencia de datos. En resumen, el enfoque de maximización de expectativas aquí consiste en el siguiente procedimiento:
\begin{itemize}
\item Adivinar algunos centros de los clúster.
\item Repetir hasta converger.
\item E-Step: asigna puntos al centro del clúster más cercano.
\item M-Step: establezca los centros de los clústers en la media.
\end{itemize}
Aquí, el ``paso E'' o ``paso de expectativa'' se llama así porque implica actualizar nuestra expectativa de a qué grupo pertenece cada punto. El ``paso M'' o ``paso de maximización'' se llama así porque implica maximizar alguna función de aptitud que define la ubicación de los centros del conglomerado; en este caso, esa maximización se logra tomando una media simple de los datos en cada conglomerado.\\
La literatura sobre este algoritmo es amplia, pero se puede resumir de la siguiente manera: en circunstancias típicas, cada repetición del paso E y del paso M siempre dará como resultado una mejor estimación de las características del grupo.\\
Se puede visualizar el algoritmo como se muestra en la siguiente figura; para la inicialización particular que se muestra aquí, los clústeres convergen en solo tres iteraciones.
\newpage
\begin{figure}[H]
	\begin{center}
	\includegraphics[width=1\textwidth]{kmeans}
		\textbf{\caption{{\small Visualización del algoritmo E-M en k-means.}}}
	{\scriptsize Fuente: [Python Data Science Handbook]}
	\end{center}
\end{figure}
El modelo K-means es un algoritmo de optimización y su objetivo es minimizar la suma de las distancias al cuadrado de cada punto de datos al centroide de su clúster. Esta métrica se llama ``inercia'' o ``suma de los cuadrados de las distancias'' y se utiliza para evaluar la calidad de los clústeres, así como Coeficiente de silueta que mide la similitud entre los puntos de datos dentro de su clúster en comparación con otros clústeres. Un coeficiente de silueta cercano a 1 indica que el punto de datos está correctamente asignado a su clúster, mientras que un coeficiente de silueta cercano a -1 indica que el punto de datos debería haber sido asignado a otro clúster.

\subsubsection{REDUCCIÓN DE LA DIMENSIONALIDAD}
\medskip
La reducción de la dimensionalidad es una técnica que se utiliza para reducir el número de variables o características en un conjunto de datos, mientras se mantiene la mayor cantidad posible de información útil. La reducción de la dimensionalidad es importante ya que a menudo los conjuntos de datos pueden contener muchas variables o características, lo que puede hacer que los modelos sean complejos y difíciles de interpretar. Además, muchos algoritmos de aprendizaje automático pueden tener dificultades para manejar conjuntos de datos con un gran número de variables.
Existen varios algoritmos de reducción de dimensionalidad, pero uno de los más populares es el análisis de componentes principales (PCA). PCA es un método lineal que utiliza una transformación matemática para encontrar una nueva representación de los datos en un espacio de menor dimensión, mientras se mantiene la mayor cantidad posible de información. La idea detrás de PCA es encontrar una nueva combinación de las variables originales que explique la mayor cantidad posible de la varianza en los datos.
\\\\
Otro algoritmo popular de reducción de la dimensionalidad es el t-distributed stochastic neighbor embedding (t-SNE). t-SNE es una técnica no lineal que se utiliza para visualizar datos en un espacio de menor dimensión. t-SNE es especialmente útil para visualizar datos de alta dimensión en dos o tres dimensiones. La técnica funciona encontrando una representación de los datos en un espacio de menor dimensión que mantiene la estructura de similitud de los datos originales.
La reducción de la dimensionalidad se utiliza en muchas aplicaciones, como la visualización de datos, la detección de anomalías y la clasificación de datos. Por ejemplo, en la visualización de datos, la reducción de la dimensionalidad se puede utilizar para visualizar datos de alta dimensión en dos o tres dimensiones. En la detección de anomalías, la reducción de la dimensionalidad se puede utilizar para identificar patrones o grupos de datos anómalos. En la clasificación de datos, la reducción de la dimensionalidad se puede utilizar para mejorar la precisión de los modelos al reducir la complejidad del conjunto de datos.
\\\\
Sin embargo, es importante tener en cuenta que la reducción de la dimensionalidad también puede tener algunas limitaciones. Por ejemplo, puede perder información importante durante el proceso de reducción de la dimensionalidad. Además, algunos algoritmos pueden ser sensibles a los valores atípicos o pueden ser difíciles de interpretar. [7]

\subsubsection{ASOCIACIÓN}
\medskip
La asociación se utiliza para encontrar patrones interesantes en conjuntos de datos grandes y complejos, para descubrir relaciones entre variables en los datos y, a menudo, se utiliza en el análisis de transacciones, como el análisis de cestas de la compra, el análisis de clics de página web y el análisis de registros de transacciones financieras.
La asociación se basa en el concepto de que los elementos en un conjunto de datos pueden estar relacionados de alguna manera. Por ejemplo, en el análisis de cestas de la compra, es posible que los clientes que compran leche también compren pan y huevos. La asociación se utiliza para encontrar patrones como este en los datos.
\\\\
Un algoritmo popular de asociación es el algoritmo Apriori. El algoritmo Apriori se utiliza para encontrar conjuntos de elementos frecuentes en un conjunto de datos. Un conjunto de elementos es frecuente si aparece con frecuencia en el conjunto de datos. El algoritmo Apriori utiliza la propiedad de que cualquier subconjunto de un conjunto frecuente también es frecuente. El algoritmo comienza encontrando los conjuntos de elementos de un solo elemento más frecuentes, luego encuentra los conjuntos de elementos de dos elementos más frecuentes, y así sucesivamente.
Otro algoritmo popular de asociación es el algoritmo FP-Growth. El algoritmo FP-Growth utiliza una estructura de árbol llamada árbol FP para encontrar conjuntos de elementos frecuentes. El árbol FP almacena los conjuntos de elementos frecuentes y sus frecuencias de manera eficiente, lo que permite que el algoritmo sea mucho más rápido que el algoritmo Apriori para conjuntos de datos grandes.
\\\\
La asociación se utiliza en muchas aplicaciones, como la recomendación de productos, la segmentación de clientes y la detección de fraudes. Por ejemplo, en la recomendación de productos, la asociación se puede utilizar para recomendar productos relacionados con los que el cliente ya ha comprado. En la segmentación de clientes, la asociación se puede utilizar para identificar grupos de clientes que tienen patrones de compra similares. En la detección de fraudes, la asociación se puede utilizar para identificar patrones de transacciones sospechosos, como un cliente que compra varios artículos caros en un corto período de tiempo.

\bigskip
\subsection{APRENDIZAJE POR REFUERZO}
\bigskip
El aprendizaje por refuerzo es una técnica de aprendizaje automático que se basa en el concepto de que un agente debe aprender a tomar decisiones óptimas en un entorno determinado para maximizar una recompensa acumulativa. En el aprendizaje por refuerzo, el agente recibe información del entorno en forma de recompensas y castigos y debe aprender a seleccionar acciones que maximicen la recompensa a largo plazo.
El agente toma una acción en un estado particular y el entorno responde con una recompensa. El objetivo del agente es aprender una política que le permita maximizar la recompensa acumulativa en el largo plazo. La política es una función que mapea estados a acciones y puede ser aprendida utilizando técnicas de aprendizaje por refuerzo.
\\\\
Un algoritmo popular de aprendizaje por refuerzo es el algoritmo Q-Learning. En Q-Learning, el agente aprende una función Q que le permite evaluar la calidad de una acción en un estado particular. La función Q se puede utilizar para seleccionar la mejor acción en un estado determinado y el algoritmo Q-Learning utiliza una técnica llamada exploración, explotación para equilibrar el aprendizaje de nuevas acciones y la selección de acciones que se sabe que son buenas.
El aprendizaje por refuerzo se utiliza en muchas aplicaciones, como los juegos, la robótica y la optimización de procesos. Por ejemplo, en los juegos, el aprendizaje por refuerzo se puede utilizar para crear agentes que aprendan a jugar juegos complejos, como el ajedrez y el Go, en la robótica, el aprendizaje por refuerzo se puede utilizar para crear robots que aprendan a realizar tareas complejas, como caminar y manipular objetos y en la optimización de procesos, el aprendizaje por refuerzo se puede utilizar para crear sistemas que aprendan a optimizar procesos, como la producción de energía y la gestión de inventarios.

%\bigskip
%\subsection{REDES NEURONALES}
%\bigskip
%Las redes neuronales son un modelo computacional inspirado en la estructura y funcionamiento del cerebro humano, estas redes están compuestas por unidades de procesamiento llamadas neuronas artificiales, que se organizan en capas y se interconectan mediante conexiones ponderadas. La información se propaga a través de la red de neuronas a través de una función de activación no lineal, lo que permite que la red realice una amplia variedad de tareas de aprendizaje.
%\\
%El proceso de entrenamiento de una red neuronal implica ajustar los valores de los pesos de las conexiones para que la salida de la red se acerque a la salida deseada. Esto se logra mediante el uso de algoritmos de aprendizaje supervisado o no supervisado. En el aprendizaje supervisado, se proporciona a la red un conjunto de datos etiquetados de entrada y salida esperada, mientras que, en el aprendizaje no supervisado, la red debe descubrir patrones en los datos de entrada por sí misma.
%Las redes neuronales han demostrado ser muy efectivas en una amplia variedad de tareas de aprendizaje automático, como la clasificación de imágenes, el reconocimiento de voz, la traducción automática y la generación de texto y música, entre otras. Además, las redes neuronales profundas, que tienen muchas capas ocultas, han llevado a grandes avances en áreas como el procesamiento del lenguaje natural y la visión por computadora.
%\\\\
%Sin embargo, el entrenamiento de redes neuronales puede ser un proceso muy intensivo en términos de recursos computacionales y de tiempo, y la interpretación de los resultados obtenidos puede ser difícil debido a la naturaleza no lineal y altamente distribuida de las redes neuronales. Además, las redes neuronales pueden ser propensas a sobreajustarse a los datos de entrenamiento, lo que puede llevar a un rendimiento deficiente en datos nuevos.
%
%\subsubsection{CONVOLUCIONALES}
%\medskip
%Las redes neuronales convolucionales son un tipo de red neuronal que ha sido especialmente diseñada para procesar datos de tipo imagen y otros datos de alto dimensionalidad. Las CNN (Convolutional Neural Network) utilizan una técnica de procesamiento conocida como convolución, que implica desplazar un filtro sobre una imagen y realizar una operación de multiplicación punto a punto entre el filtro y la sección de la imagen correspondiente.
%\\\\
%Las CNN son especialmente útiles para tareas como la clasificación de imágenes y la detección de objetos, ya que son capaces de extraer características de las imágenes y otros datos de alto dimensionalidad con gran precisión, estas características se utilizan luego para realizar la clasificación o detección de objetos, según sea el caso. Las CNN son particularmente efectivas en la detección de patrones en datos de tipo imagen, ya que los filtros convolucionales permiten capturar características locales de la imagen, como bordes, texturas y patrones repetitivos, de manera eficiente. Además, las CNN pueden aprender automáticamente las características relevantes de los datos de entrenamiento, lo que las hace muy efectivas para tareas de clasificación y detección de objetos en las que las características relevantes no son conocidas de antemano.
%Aunque las CNN son particularmente útiles para tareas de procesamiento de imagen, también se han utilizado con éxito en tareas como el procesamiento del lenguaje natural, la clasificación de secuencias de tiempo y la detección de anomalías. Además, las redes neuronales convolucionales profundas, que contienen varias capas de convolución, han llevado a grandes avances en áreas como el procesamiento de imágenes médicas y la visión por computadora. [8]
%
%\subsubsection{RECURRENTES}
%\medskip
%Las redes neuronales recurrentes (RNN, recurrent neural network) son un tipo de red neuronal que se utiliza para procesar datos secuenciales, como el lenguaje natural y las series de tiempo. A diferencia de las redes neuronales convolucionales, que procesan los datos de manera independiente en cada posición, las RNN tienen memoria y son capaces de procesar secuencias de datos de longitud variable.
%En una red neuronal recurrente, cada neurona tiene una conexión consigo misma, lo que permite que la información fluya a través de la red en bucles. Esta arquitectura de bucle permite que la red neuronal recuerde información de los pasos anteriores y la utilice para procesar la entrada actual.
%La principal ventaja de las RNN es su capacidad para modelar la dependencia temporal de los datos. Esto significa que la red puede aprender patrones en secuencias de datos, como la estructura sintáctica del lenguaje natural o los patrones de fluctuación en una serie de tiempo. Además, las RNN son capaces de procesar secuencias de datos de longitud variable, lo que las hace muy útiles para tareas como la traducción automática, donde la longitud de la entrada y la salida puede variar.
%\\\\
%Una variante de las RNN son las redes neuronales LSTM (Long Short-Term Memory), que fueron diseñadas para evitar el problema del desvanecimiento del gradiente, que puede ocurrir cuando se entrena una red neuronal recurrente profunda. Las redes LSTM utilizan un mecanismo de compuertas para controlar el flujo de información a través de la red y evitar que la señal degradada afecte al entrenamiento de la red.
%Las redes neuronales recurrentes y las redes LSTM han tenido un gran éxito en una amplia variedad de aplicaciones, incluyendo la generación de texto, la traducción automática, el reconocimiento de voz y la predicción de series de tiempo. Sin embargo, las redes neuronales recurrentes también tienen algunas limitaciones, como la dificultad para manejar dependencias a largo plazo y el costo computacional elevado de entrenar redes profundas. [9]
%
%\subsubsection{DE ATENCIÓN}
%\medskip
%Las redes neuronales de atención (attention-based neural networks) son un tipo de red neuronal que se utiliza para procesar datos secuenciales y modelar la dependencia temporal de los datos, al igual que las redes neuronales recurrentes. Sin embargo, a diferencia de las redes neuronales recurrentes, las redes neuronales de atención no procesan todos los datos secuenciales de manera uniforme, sino que prestan atención a partes específicas de la secuencia en cada paso de procesamiento.
%En una red neuronal de atención, cada paso de procesamiento se divide en dos partes: la codificación y la decodificación. Durante la codificación, la red procesa la entrada secuencial y la transforma en una serie de vectores de características. Durante la decodificación, la red genera la salida secuencial a partir de los vectores de características generados durante la codificación.
%\\
%La atención se utiliza para determinar qué vectores de características se deben utilizar en cada paso de decodificación. En lugar de procesar toda la secuencia de entrada en cada paso de decodificación, la red presta atención a las partes más relevantes de la entrada en cada paso, utilizando una función de atención para asignar pesos a cada vector de características de la secuencia de entrada.
%\\\\
%La principal ventaja de las redes neuronales de atención es su capacidad para enfocarse en las partes más importantes de la entrada secuencial en cada paso de procesamiento, lo que las hace muy útiles para tareas como la traducción automática, donde la atención se puede utilizar para identificar las partes más relevantes de la oración en la que se está trabajando en cada paso del proceso de traducción.
%Una variante de las redes neuronales de atención son las redes neuronales de atención múltiple (multi-head attention networks), que utilizan múltiples funciones de atención para procesar diferentes aspectos de la entrada secuencial de manera simultánea.
%\\\\
%Las redes neuronales de atención han tenido un gran éxito en una amplia variedad de aplicaciones, incluyendo la traducción automática, el procesamiento del lenguaje natural y la generación de texto. Sin embargo, al igual que todas las redes neuronales, también tienen algunas limitaciones que deben ser consideradas, como la necesidad de grandes cantidades de datos de entrenamiento y el costo computacional elevado de entrenar redes profundas.

\bigskip
\subsection{SERIES TEMPORALES}
\bigskip
Una serie temporal es una colección de datos de una variable recogidas secuencialmente en el tiempo, estos datos de series temporales siguen intervalos de tiempo periódicos que se midieron en intervalos de tiempo regulares o se recopilaron en intervalos de tiempo particulares. Estos datos se suelen recoger en instantes de tiempo equiespaciados, si los datos se recogen en instantes temporales de forma continua, se debe o bien digitalizar la serie, es decir, recoger sólo los valores en instantes de tiempo equiespaciados, o bien acumular los valores sobre intervalos de tiempo.\\ 
Los datos de series temporales siguen intervalos de tiempo periódicos que se midieron en intervalos de tiempo regulares o se recopilaron en intervalos de tiempo particulares. En decir una serie temporal es simplemente una serie de puntos de datos ordenados en el tiempo, y el análisis de series temporales es el proceso de dar sentido a dichos datos. [16]
\\\\
Las series temporales pueden ser descompuestas en varios componentes, que ayudan a entender mejor la estructura de los datos y a modelarlos de manera adecuada. Los componentes comunes de una serie temporal son los siguientes:
\begin{itemize}
\item \textbf{Tendencia:} Es la dirección general de los datos a largo plazo. Puede ser creciente, decreciente o constante. La tendencia indica la dirección en la que se mueven los datos en el largo plazo.
\item \textbf{Estacionalidad:} Son patrones que se repiten en un intervalo de tiempo fijo, como las estaciones del año, días de la semana, horas del día, etc. La estacionalidad indica cómo los datos varían a corto plazo, en períodos fijos.
\item \textbf{Cíclico:} Son patrones que se repiten en intervalos irregulares, generalmente más largos que los patrones estacionales. Los ciclos pueden ser causados por factores económicos, sociales o políticos.
\item \textbf{Componente aleatorio:} Es la variación aleatoria en la serie temporal que no puede ser explicada por la tendencia, la estacionalidad o los ciclos. Este componente representa la variabilidad en la serie temporal que no puede ser explicada por otros factores.
\end{itemize}

La descomposición de una serie temporal en estos componentes se puede hacer utilizando técnicas estadísticas como el análisis de series de tiempo o métodos más avanzados como el análisis de componentes principales. La comprensión de estos componentes es importante para modelar adecuadamente la serie temporal y hacer predicciones precisas. Por ejemplo, si se espera que la tendencia y la estacionalidad se mantengan constantes, se puede utilizar un modelo ARIMA para modelar la serie temporal. Si los datos tienen ciclos irregulares, un modelo de regresión de series temporales puede ser más apropiado.

\subsubsection{FORECASTING AUTORREGRESIVO RECURSIVO (RAF)}
\medskip
Es un modelo de pronóstico de series temporales que utiliza un enfoque recursivo para predecir valores futuros, se basa en la idea de que los valores futuros de una serie temporal están altamente correlacionados con sus valores pasados, y se puede utilizar esta relación para predecir valores futuros de manera recursiva.
El modelo RAF se compone de dos partes principales: la primera parte es un modelo autorregresivo (AR) que estima la relación entre los valores pasados de la serie temporal y su valor actual, mientras que la segunda parte es una función recursiva que utiliza el modelo AR para predecir valores futuros de manera recursiva. [18]\\ 
El modelo AR se basa en la idea de que el valor actual de la serie temporal está relacionado linealmente con sus valores pasados, y se puede modelar mediante una ecuación matemática. La ecuación AR se puede escribir como:
\begin{equation}
y_t = c + \sum\limits_{i=1}^{n} \varphi_i \cdot y_{t-i} + e_t
\end{equation}
donde:
\\\\
$y_t$ \hspace{0.52em} es el valor actual de la serie temporal.\\
$e_t$ \hspace{0.52em} es el término de error aleatorio.\\
$c$ \hspace{1em} es una constante.\\
$\varphi_i$ \hspace{0.4em} son los coeficientes de la ecuación AR que representan la relación entre los valores pasados y el valor actual.
\\\\
Una vez que se ha ajustado el modelo AR a los datos de la serie temporal, se utiliza la función recursiva para predecir los valores futuros de la serie temporal. La función recursiva se define como:
\begin{equation}
y_{(t+h)} = c + \sum\limits_{i=1}^{n} \varphi_i \cdot y_{(t+h-i)}
\end{equation}
donde:
\\\\
$y_{(t+h)}$ \hspace{0.6em} es el valor predicho de la serie temporal en el momento $t+h$.\\
$y_{t-i}$ \hspace{1.35em} es el valor pasado de la serie temporal en el momento $t-i$.
\\\\
Esta función se utiliza para predecir los valores futuros de la serie temporal recursivamente, utilizando los valores pasados de la serie temporal.
El modelo RAF se puede utilizar para pronosticar diferentes horizontes de pronóstico, es decir, el modelo puede utilizarse para pronosticar los valores futuros de la serie temporal a corto plazo, a medio plazo o a largo plazo. El modelo es especialmente útil para pronósticos de corto plazo, ya que se basa en la información disponible en el momento actual y utiliza una función recursiva para predecir valores futuros.

\bigskip
\subsection{ESTUDIOS Y MODELOS ACTUALES DE COVID-19}
\bigskip
%El aprendizaje automático ha experimentado un gran avance en las últimas décadas gracias al aumento de la cantidad y calidad de datos disponibles y a la mejora de los algoritmos uno de los más importante ha sido el desarrollo de algoritmos de aprendizaje semi-supervisado y de transferencia de aprendizaje, que permiten entrenar modelos con conjuntos de datos más pequeños y reducir el tiempo y los costos de entrenamiento. Estos enfoques son especialmente útiles en áreas donde la recopilación de datos es costosa o difícil, como en la medicina o la astronomía.
%\\\\
%A su vez el aprendizaje federado es una técnica de aprendizaje supervisado que ha ganado mucha atención en los últimos años debido a su capacidad para entrenar modelos de manera distribuida y colaborativa sin la necesidad de compartir los datos subyacentes. Esto lo hace especialmente útil en situaciones donde la privacidad y la seguridad son una preocupación importante, como en el análisis de datos médicos o financieros. Un ejemplo es el proyecto NVIDIA FLARE, un kit de desarrollo de software que ayuda a las partes distribuidas a colaborar para desarrollar modelos de IA más generalizables \textit{``El código abierto de NVIDIA FLARE para acelerar la investigación del aprendizaje federado es especialmente importante en el área de la salud, donde el acceso a conjuntos de datos multiinstitucionales es crucial, pero las preocupaciones sobre la privacidad del paciente pueden limitar la capacidad de compartir datos''}, Dr. Jayashree Kalapathy. [10]
%\\
%Para nuestro proyecto el aprendizaje federado no es el adecuado ya que no contamos con información sensible y la seguridad de esta información no es relevante, ya que son datos públicos suministrados por el gobierno y páginas oficiales.
%\\\\
%Sin duda el área en donde más se ha experimentado grandes avances ha sido las redes neuronales especialmente en áreas como las redes neuronales profundas, las arquitecturas de redes neuronales innovadoras (diversas arquitecturas que abordan problemas específicos), el aprendizaje por transferencia el cual implica reutilizar las capas ocultas de una red pre-entrenada en una tarea y adaptarla para una tarea nueva. Así como nuevas técnicas para mejorar los modelos como lo es la técnica de regularización que se utilizan para evitar el sobreajuste o el sobreentrenamiento de las redes neuronales, esto implica agregar términos de penalización a la función de costo de la red para evitar que los pesos de la red adquieran valores extremos. Algunos proyectos recientes en estas áreas son:
%
%\begin{itemize}
%
%\item \textbf{AlphaGo:} Es un programa de ordenador desarrollado por DeepMind que utiliza una combinación de redes neuronales y algoritmos de búsqueda para jugar al juego de mesa chino Go. En 2016, AlphaGo se convirtió en el primer programa de ordenador en derrotar a un campeón humano de Go, lo que fue considerado un hito importante en la inteligencia artificial. [11]
%
%\item \textbf{DALL-E:} Es un modelo de red neuronal desarrollado por OpenAI que genera imágenes a partir de descripciones de texto. El modelo utiliza una combinación de redes neuronales convolucionales y de atención para generar imágenes realistas y detalladas a partir de descripciones de texto. [12]
%
%\begin{figure}[H]
%	\begin{center}
%	\includegraphics[width=0.7\textwidth]{astronauta}
%		\textbf{\caption{{\small Astronauta sosteniendo una flor - DALL-E.}}}
%	{\scriptsize Fuente: [Dall-E 2: Why the AI image generator is a revolutionary invention]}
%	\end{center}
%\end{figure}
%\item \textbf{StyleGAN2:} Es un modelo de red neuronal que se utiliza para generar imágenes fotorrealistas de alta calidad. El modelo utiliza una técnica llamada "red neuronal generativa adversarial" para generar imágenes que son indistinguibles de las fotos reales. [13]
%
%\begin{figure}[H]
%	\begin{center}
%	\includegraphics[width=0.5\textwidth]{stylegan2}
%		\textbf{\caption{{\small Reconstrucción de imagen - StyleGAN2.}}}
%	{\scriptsize Fuente: [MIT CSAIL Uses Deep Generative Model StyleGAN2 to Deliver SOTA Image Reconstruction Results]}
%	\end{center}
%\end{figure}
%\item \textbf{BERT (Bidirectional Encoder Representations from Transformers):} Es un modelo de lenguaje natural basado en redes neuronales de atención que ha demostrado una gran precisión en una variedad de tareas de procesamiento de lenguaje natural, como la clasificación de texto y la respuesta a preguntas.
%
%\end{itemize}
%
%Estos avances muestran cómo las redes neuronales están transformando la forma en que las computadoras pueden aprender y procesar información, abriendo nuevas posibilidades en áreas como la generación de lenguaje natural, la visión por computadora y la toma de decisiones inteligente. Para nuestro caso puntual no se utilizarán estas técnicas recientes debido al alcance del proyecto, en donde se evaluará modelos tradicionales y la comparación entre estos.
El aprendizaje automático ha experimentado un gran avance en las últimas décadas gracias al aumento de la cantidad y calidad de datos disponibles y a la mejora de los algoritmos uno de los más importante ha sido el desarrollo de algoritmos de aprendizaje semi-supervisado, redes neuronales y de transferencia de aprendizaje, que permiten entrenar modelos con conjuntos de datos más pequeños y reducir el tiempo y los costos de entrenamiento. Estos enfoques son especialmente útiles en áreas donde la recopilación de datos es costosa o difícil, como en la medicina (enfermedades, pandemias, etc.) o la astronomía.
Desde que el COVID-19 comenzó a propagarse por todo el mundo, se han desarrollado una variedad de modelos de predicción para ayudar a los responsables de la toma de decisiones a entender mejor la propagación del virus y tomar decisiones informadas para proteger a la población. Estos estudios han surgido de diferentes áreas como lo son: Modelos epidemiológicos que se basan en la teoría de la propagación de enfermedades y utilizan datos de casos confirmados, fallecimientos, y otros factores como la edad, el género y las comorbilidades para predecir la propagación futura del virus, los modelos epidemiológicos más comunes son SIR (Susceptible-Infectado-Recuperado) y SEIR (Susceptible-Expuesto-Infectado-Recuperado); modelos de aprendizaje automático que utilizan algoritmos para analizar datos y hacer predicciones basadas en patrones; los modelos basados en la movilidad que utilizan datos de movilidad de los teléfonos móviles y otros dispositivos para predecir la propagación del COVID-19; entre otros. Haciendo un examen más profundo en el área de interés de este trabajo se encuentran investigaciones bastante interesantes detalladas a continuación: 
\begin{itemize}
\item \textbf{Modelos basados en series de tiempo:} Estos modelos utilizan datos históricos del COVID-19 para predecir la propagación futura de la enfermedad. Un ejemplo es el modelo ARIMA (Autoregressive Integrated Moving Average) que se ha utilizado para predecir la propagación del COVID-19 en varios países [20].
\item \textbf{Modelos basados en redes neuronales:} Las redes neuronales son capaces de aprender patrones complejos en los datos y han sido utilizadas para predecir la propagación del COVID-19. Por ejemplo, el modelo LSTM (Long Short-Term Memory) ha sido utilizado para predecir el número de casos confirmados en Canada, Estados Unidos e Italia [21]. \href{https://doi.org/10.1016/j.chaos.2020.109864}{Time series forecasting of COVID-19 using LSTM networks}
\item \textbf{Modelos basados en análisis de redes:} Estos modelos utilizan técnicas de análisis de redes para modelar la propagación del COVID-19. Un ejemplo es el modelo SEIR (Susceptible-Exposed-Infected-Recovered) que modela la propagación del virus como una red de interacciones entre individuos [22]. Los autores utilizaron datos públicos de COVID-19 en Jordania para calibrar y validar el modelo. Luego, el modelo se utilizó para predecir la propagación del virus bajo diferentes escenarios de intervención, como el cierre de escuelas, el cierre de negocios y la cuarentena de casos sospechosos y confirmados. Los resultados del estudio muestran que el modelo puede predecir con precisión la propagación del COVID-19 en Jordania bajo diferentes escenarios de intervención. Los autores también encontraron que el cierre de escuelas y negocios, junto con la cuarentena de casos sospechosos y confirmados, puede reducir significativamente la propagación del virus en Jordania.
\item \textbf{Modelos basados en aprendizaje automático:} Los modelos de aprendizaje automático utilizan técnicas de machine learning para predecir la propagación del COVID-19. Un ejemplo es el modelo XGBoost que ha sido utilizado para predecir la propagación del virus en varios países [23]. Este propone un nuevo enfoque híbrido de aprendizaje profundo (DL) para estimar los patrones de transmisión de COVID-19 en Corea del Sur. El marco propuesto combina el aprendizaje profundo con el modelo de meta población susceptible-expuesto-infectado-recuperado (SEIR). Para mostrar su eficacia, el marco híbrido de aprendizaje profundo se comparó con el modelo de memoria a corto plazo (LSTM) y el modelo general de red neuronal profunda (DNN) para pronosticar patrones epidémicos en Corea del Sur en función del mismo conjunto de datos. El análisis numérico demostró que el marco híbrido de aprendizaje profundo que utiliza el modelo de meta población y el modelo LSTM exhibe el mejor rendimiento entre los métodos de prueba. 
\end{itemize}
Fuera de los ejemplos expuestos anteriormente sobresale el proyecto \textbf{COVID-19 Projections} de Estados Unidos, que utiliza técnicas de aprendizaje automático para predecir la propagación del COVID-19 en línea (\url{https://covid19-projections.com/}) y se actualiza regularmente con nuevos datos. Este proyecto está desarrollado un simulador basado en el modelo \href{https://en.wikipedia.org/wiki/Compartmental_models_in_epidemiology#The_SEIR_model}{SEIR} para simular la epidemia de COVID-19 en cada región. Luego, los parámetros/entradas de este simulador se aprenden mediante técnicas de aprendizaje automático que intentan minimizar el error entre las salidas proyectadas y los resultados reales. Utiliza los datos diarios de muertes informados por cada región para pronosticar futuras muertes informadas. Después de algunas técnicas de validación adicionales (para minimizar un fenómeno llamado sobreajuste), se usa los parámetros aprendidos para simular el futuro y hacer proyecciones.
\\\\
Este modelo SEIR es de \href{https://github.com/youyanggu/yyg-seir-simulator}{código abierto} . Las proyecciones se cargan diariamente en \href{https://github.com/youyanggu/covid19_projections/tree/master/projections}{GitHub}. El objetivo de este proyecto es mostrar las fortalezas de la inteligencia artificial para abordar uno de los problemas más difíciles del mundo: predecir la trayectoria de una pandemia. En donde se utiliza un enfoque basado en datos puros al dejar que la máquina aprenda. Actualmente esta haciendo proyecciones para: Estados Unidos, los 50 estados de EE. UU. (más DC, PR, VI, GU, MP) y 70 países (incluidos los 27 países de la UE). Combinados, estos 71 países representan más del 95\% de todas las muertes globales por COVID-19.
\\\\
Sin duda hay una gran variedad de estudios, artículos y publicaciones que tratan los factores principales en la propagación del covid-19, pero se ha dejado de lado el estudio sobre otros factores que podrían influir en la propagación, como lo es el factor climático; centro de estudio de este trabajo. 

\newpage
\begin{center}
\section{DESARROLLO DEL PROYECTO Y RESULTADOS}
\end{center}
Para el desarrollo del proyecto se utilizó la metodología KDD (Knowledge Discovery in Databases) en los distintos orígenes de datos, el objetivo de es crear un set de datos refinado a partir de todos los orígenes y a partir de este extraer conocimiento a través de los modelos creados para darle una solución al planteamiento del problema. Se realizará un análisis sobre los resultados obtenidos y las posibles mejoras en trabajos futuros.
\\\\
Todo el desarrollo del proyecto se realizará en lenguaje Python, a partir de notebooks de Jupyter, en estos se encontrará el desarrollo de todas las etapas del proceso de KDD y el porque se toman ciertas decisiones. Basado en los resultados de los notebooks se crean archivos .py con algunas etapas para realizar una CI/CD. Las fuentes de datos, los notebooks con los análisis de cada etapa y los archivos .py para la automatización de todo el flujo se encuentra en el repositorio personal para este TFM, a su vez se encontrará el código fuente de LaTeX el cual se desarrolló el presente documento: 
\href{https://github.com/Danilo0221/TFM}{REPOSITORIO TFM}


\bigskip
\subsection{METODOLOGÍA}
\medskip
KDD (Knowledge Discovery in Databases), o Descubrimiento de Conocimiento en Bases de Datos, es un proceso integral que implica la identificación, extracción y transformación de patrones y conocimientos valiosos a partir de grandes conjuntos de datos. Es una disciplina que combina el uso de técnicas de minería de datos, estadísticas, aprendizaje automático y bases de datos para descubrir información útil y conocimientos ocultos en datos no estructurados o estructurados.
\\
\begin{figure}[H]
	\begin{center}
	\includegraphics[width=1\textwidth]{KDD2}
		\textbf{\caption{{\small Descripción de los pasos que constituyen el proceso KDD}}}
	{\scriptsize Fuente: [The KDD Process for Extracting Useful Knowledge from Volumes of Data]}
	\end{center}
\end{figure}

El proceso de KDD consta de varias etapas, que incluyen:
\begin{itemize}
\item \textbf{Selección de datos:} Consiste en la identificación y recopilación de los datos relevantes para el análisis. Esto puede involucrar la obtención de datos de diversas fuentes, la limpieza y preprocesamiento de los datos para asegurar su calidad y consistencia.
\item \textbf{Preprocesamiento de datos:} Implica la transformación y limpieza de los datos para prepararlos para el análisis. Esto puede incluir la eliminación de datos duplicados o inconsistentes, la normalización de los datos, la imputación de valores faltantes y la selección de características relevantes.
\item \textbf{Transformación de datos:} Involucra la conversión de los datos preprocesados en formatos adecuados para el análisis. Esto puede incluir la transformación de datos categóricos en datos numéricos, la discretización de datos continuos, la reducción de dimensionalidad, entre otros.
\item \textbf{Minería de datos:} Es la etapa central de KDD, donde se aplican técnicas y algoritmos de minería de datos para descubrir patrones y conocimientos en los datos. Esto puede incluir técnicas de clasificación, regresión, agrupamiento, asociación, entre otras.
\item \textbf{Interpretación y Evaluación de resultados:} Implica la interpretación y comunicación de los resultados obtenidos a través de técnicas de visualización y presentación de datos. Así como la utilización de métricas de evaluación y validación para medir la precisión, el rendimiento y la utilidad de los resultados obtenidos. Esto puede ayudar a comprender y utilizar los patrones y conocimientos descubiertos para tomar decisiones informadas y mejorar la toma de decisiones en diversas áreas de aplicación.
\end{itemize}

\bigskip
\subsection{PLANTEAMIENTO DEL PROBLEMA}
\medskip
El COVID-19 es una enfermedad infecciosa altamente contagiosa que ha afectado a millones de personas en todo el mundo y ha causado la muerte de cientos de miles de personas. Si bien se sabe que la propagación del virus se produce principalmente por contacto cercano con personas infectadas, también hay evidencia emergente que sugiere que las condiciones climáticas como la temperatura, la humedad y la luz solar, pueden influir en la propagación del virus.  Un modelo de machine learning puede ser una herramienta útil para evaluar la relación entre las condiciones climáticas y la propagación del COVID-19. El objetivo de este planteamiento del problema es desarrollar un modelo de machine learning que pueda predecir la propagación del virus en función de factores climáticos como la temperatura, la humedad y la luz solar.
\\
El modelo podría utilizar datos históricos sobre la propagación del virus y las condiciones climáticas para predecir la propagación futura del virus en diferentes condiciones climáticas o más específicamente dependiendo del modelo utilizado a groso modo se podrían utilizar de la siguiente forma:
\\
\begin{itemize}
\item \textbf{Modelo de regresión:} Utilizando datos históricos de propagación del virus y condiciones climáticas para predecir la propagación futura del virus. El modelo puede incluir variables relacionadas con la propagación del virus, como el número de casos confirmados y la tasa de reproducción.
\item  \textbf{Redes neuronales:} Se podría utilizar para analizar grandes conjuntos de datos de propagación del virus y condiciones climáticas. El modelo puede aprender patrones y relaciones entre las variables para predecir la propagación futura del virus en diferentes condiciones climáticas.
\item  \textbf{Análisis de series de tiempo:} Si contamos con datos históricos para identificar patrones y tendencias en la propagación del virus y las condiciones climáticas. El modelo puede predecir la propagación futura del virus en función de los patrones identificados.
\item  \textbf{Modelos de aprendizaje profundo:} Si tenemos datos de satélite y mapas climáticos para predecir la propagación del virus. Estos modelos pueden integrar datos climáticos con información sobre la densidad de población y la movilidad humana para predecir cómo se propagará el virus en diferentes áreas geográficas.
\end{itemize}
\bigskip
Lo anterior es solo un bosquejo de un posible uso de cada tipo de modelo, conforme vayamos avanzando en nuestra investigación determinaremos cuál es el modelo que más se ajusta a nuestro requerimiento o que obtenga mejores resultados basado en sus métricas, el resultado de esto podría ayudar a informar la toma de decisiones sobre políticas de salud pública y permitir a las autoridades sanitarias tomar medidas preventivas antes de que se produzca un aumento en los casos de COVID-19.
Al responder a esta pregunta, se pueden desarrollar mejores estrategias de prevención y mitigación para el COVID-19, y se pueden aplicar los hallazgos a futuras pandemias y enfermedades infecciosas.

\bigskip
\subsection{DESARROLLO DEL PROYECTO}
\medskip
El proyecto se desarrollará siguiendo cada una de las etapas del KDD, ya que proporciona una estructura sistemática para la extracción de conocimiento a partir de los datos. Al seguir cada una de las etapas del KDD, se puede asegurar que el proceso es riguroso y que se obtienen resultados precisos y relevantes para el proyecto.

\subsubsection{SELECCIÓN DE DATOS}
Para la ejecución del proyecto se utilizaron distintas fuentes de datos las cuales se detallan a continuación:
\begin{itemize}
\item \textbf{Datos casos COVID-19 por provincias} 
\\\\
Los resultados que se presentan se obtienen a partir de la declaración de los casos de COVID-19 a la Red Nacional de Vigilancia Epidemiológica (RENAVE) a través de la plataforma informática vía Web SiViES (Sistema de Vigilancia de España) que gestiona el Centro Nacional de Epidemiología (CNE). Esta información procede de la encuesta epidemiológica de caso que cada Comunidad Autónoma cumplimenta ante la identificación de un caso de COVID-19. Datos oficiales disponibles en el sitio web: \url{https://cnecovid.isciii.es/covid19/#documentaci%C3%B3n-y-datos}
\\
Se utilizan los siguientes sets de datos:
\\\\
\textbf{\textit{casos\_hosp\_uci\_def\_sexo\_edad\_provres.csv:}} Datos desde el inicio de la pandemia, para todas las edades, hasta el 28 de marzo de 2022.
\\
\textbf{\textit{casos\_hosp\_uci\_def\_sexo\_edad\_provres\_60\_mas.csv:}} Datos desde el inicio de la pandemia, para la población de 60 o más años.
\\\\
Para ambos casos cuentan con las mismas columnas, así como su descripción. Esta descripción se detalla en la tabla \textbf{\ref{tb:testTable}} del anexo I.

\item \textbf{Datos códigos provincias} 
\\\\
Archivo de elaboración propia que contiene el nombre de la provincia y el correspondiente código ISO de la provincia. Esta fuente de datos es necesaria para el cruce de información o unión de los sets de datos, ya que algunas fuentes tienen el nombre de la provincia y otros tiene su correspondiente código ISO de cada provincia.

\begin{table}[H]
  \centering
   \textbf{\caption{{\small Código ISO y nombre de provincia. \label{tb:Tablecodprov}}}}       
	\renewcommand{\arraystretch}{1.2}  
  %{\setlength{\arrayrulewidth}{0.3mm}
    \begin{tabular}{|c|p{8cm}|}
    \hline
    \rowcolor{Peach} Variable & Descripción \\
	\hline    
    PROVINCIA\_ISO & Código ISO correspondiente a la provincia \\
    \hline
    PROVINCIA & Nombre de la provincia \\
    \hline
    \end{tabular}
    \label{tab:addlabel}\\
  {\scriptsize Fuente: Elaboración Propia}
\end{table}

\item \textbf{Datos climatológicos por provincia}
\\\\
Los datos climatológicos son diarios por provincia, estos datos oficiales fueron extraídos desde el portal datos abiertos de AEMET, que permite la difusión y la reutilización de la información meteorológica y climatológica de la Agencia, en el sentido indicado en la \href{https://www.boe.es/boe/dias/2015/07/10/pdfs/BOE-A-2015-7731.pdf}{Ley 18/2015, de 9 de julio}, por la que se modifica la Ley 37/2007, de 16 de noviembre, sobre reutilización de la información del sector público. Para poder acceder a AEMET OpenData, es necesario solicitar una API Key (https://opendata.aemet.es/centrodedescargas/altaUsuario?). Una API Key es un identificador, mediante el cual se contabilizan e imputan los accesos que un usuario realiza al API. Mediante el API KEY solicitado se obtiene la data en el siguiente sitio web: \url{https://opendata.aemet.es/centrodedescargas/productosAEMET}. 
%La información viene en un archivo .json por provincia, teniendo así 52 archivos .json con la información climatológica diaria correspondiente. 
\newpage
\begin{table}[H]
  \centering
   \textbf{\caption{{\small Variables climatológicas de provincia. \label{tb:Tableclimaprov}}}}       
	\renewcommand{\arraystretch}{1.2}  
  %{\setlength{\arrayrulewidth}{0.3mm}
    \begin{tabular}{|c|p{12cm}|}
    \hline
    \rowcolor{Peach} Variable & Descripción \\
	\hline    
    fecha & fecha del dia (AAAA-MM-DD) \\
    \hline
    indicativo & indicativo climatológico \\
    \hline
    nombre & nombre (ubicación) de la estación \\
    \hline
    provincia & provincia de la estación \\
    \hline
    altitud & altitud de la estación en m sobre el nivel del mar \\
    \hline
    tmed & Temperatura media diaria \\
    \hline
    prec & Precipitación diaria de 07 a 07 \\
    \hline
    tmin & Temperatura Mínima del día \\
    \hline
    horatmin & Hora y minuto de la temperatura mínima \\
    \hline
    tmax & Temperatura Máxima del día \\
    \hline
    horatmax & Hora y minuto de la temperatura máxima \\
    \hline
    dir & Dirección de la racha máxima \\
    \hline
    velmedia & Velocidad media del viento \\
    \hline
    racha & Racha máxima del viento \\
    \hline
    horaracha & Hora y minuto de la racha máxima \\
    \hline
    sol & Insolación \\
    \hline
    presmax & Presión máxima al nivel de referencia de la estación \\
    \hline
    horapresmax & Hora de la presión máxima (redondeada a la hora entera más próxima) \\
    \hline
    presmin & Presión mínima al nivel de referencia de la estación \\
    \hline
    horapresmin & Hora de la presión mínima (redondeada a la hora entera más próxima) \\
    \hline
    \end{tabular}
    \label{tab:addlabel}\\
  {\scriptsize Fuente: Elaboración Propia}
\end{table}

\item \textbf{Datos población por provincia}
\\\\
Los datos anuales demográficos por provincia fueron extraídos desde el instituto nacional de estadística de España, mediante el sitio web: \url{https://www.ine.es/jaxi/Datos.htm?path=/t20/e245/p08/l0/&file=03003.px#!tabs-tabla}, esta fuente de datos es necesaria para realizar una normalización de los casos de covid-19 por provincia o lo que llamamos la tasa de incidencia.
\begin{table}[H]
  \centering
   \textbf{\caption{{\small Población anual desde 1998 por provincia. \label{tb:Tablepoblaprov}}}}       
	\renewcommand{\arraystretch}{1.2}  
  %{\setlength{\arrayrulewidth}{0.3mm}
    \begin{tabular}{|c|p{8cm}|}
    \hline
    \rowcolor{Peach} Variable & Descripción \\
	\hline    
    Provincias & Nombre de la provincia \\
    \hline
    Sexo & H (hombre), M (mujer), Ambos sexos \\
    \hline
    Edad (año a año) & Rango de edad - total edades (contempla todas) \\
    \hline
    Españoles/Extranjeros & Origen de la persona - total (comtempla ambos) \\
    \hline
    Año & Año de recopilación de la información \\
    \hline
    Total & Cantidad de personas \\
    \hline    
    \end{tabular}
    \label{tab:addlabel}\\
  {\scriptsize Fuente: Elaboración Propia}
\end{table}

\end{itemize}

\subsubsection{PREPROCESAMIENTO DE DATOS}
Para cada una de las fuentes de datos se realiza el procesamiento de su información, así como el dataset total que esta compuesto de la unión de estás fuentes mencionados en el apartado anterior.
\begin{itemize}
\item \textbf{Datos casos COVID-19 por provincias}
\\\\
Ambos sets de datos de covid-19 por provincia cuenta con 8 columnas con los mismos nombres que fueron descritas en el apartado anterior. Se realiza la unión de ambos set de datos (\textit{casos\_hosp\_uci\_def\_sexo\_edad\_provres.csv} y \\ \textit{casos\_hosp\_uci\_def\_sexo\_edad\_provres\_60\_mas.csv}), ya que la única diferencia entre estos dos es el rango de edad que presenta en una de sus variables. Se realiza un perfilamiento de los datos mediante la librería de \href{https://pypi.org/project/pandas-profiling/}{pandas\_profiling} el cual a partir de un dataframe de pandas genera el perfilamiento de la data en el siguiente archivo \href{https://htmlpreview.github.io/?https://github.com/Danilo0221/TFM/blob/main/DataCleaning/df_covid_prof.html}{df\_covid\_prof.html}.\\
Realizamos un análisis de la cantidad de nulos en todas sus variables mediante un heatmap.

\begin{figure}[H]
	\begin{center}
	\includegraphics[width=1\textwidth]{heatmap_covid}
		\textbf{\caption{{\small Headmap variables dataset covid-19.}}}
	{\scriptsize Fuente: [Elaboración propia]}
	\end{center}
\end{figure}

Del gráfico anterior observamos que no se evidencian nulos, pero es una medida cualitativa ya que por la cantidad de datos pueda que haya muy pocos y no se evidencien, por lo que se procede a hacer una lista de porcentaje de valores nulos:

\begin{figure}[H]
	\begin{center}
	\includegraphics[width=0.3\textwidth]{list_nulos_covid}
		\textbf{\caption{{\small Porcentaje de nulos dataset covid-19.}}}
	{\scriptsize Fuente: [Elaboración propia]}
	\end{center}
\end{figure}
Los estadísticos obtenidos para el dataset se representan en la siguiente imagen:

%\begin{figure}[H]
%	\begin{center}
%	\includegraphics[width=0.4\textwidth]{describe_covid}
%		\textbf{\caption{{\small Estadísticos dataset covid-19.}}}
%	{\scriptsize Fuente: [Elaboración propia]}
%	\end{center}
%\end{figure}

%\begin{table}[H]
%  \centering
%   \textbf{\caption{{\small Estadísticos dataset covid-19. \label{tb:TableEstadCovid}}}}       
%	\renewcommand{\arraystretch}{1.2}  
%  %{\setlength{\arrayrulewidth}{0.3mm}
%    \begin{tabular}{|c|c|c|c|c|}
%    \hline
%    \rowcolor{Peach}  & \textbf{num\_casos} & \textbf{num\_hosp} & \textbf{num\_uci} & \textbf{num\_def} \\
%	\hline    
%    \textbf{count} & 1461210.00 & 1461210.00 & 1461210.00 & 1461210.00\\
%    \hline
%    \textbf{mean} & 8.74 & 0.43 & 0.04 & 0.08\\
%    \hline
%    \textbf{std} & 48.38 & 2.49 & 0.30 & 0.77\\
%    \hline
%    \textbf{min} & 0.00 & 0.00 & 0.00 & 0.00\\
%    \hline
%    \textbf{25 \%} & 0.00 & 0.00 & 0.00 & 0.00\\
%    \hline 
%    \textbf{50 \%} & 0.00 & 0.00 & 0.00 & 0.00\\
%    \hline 
%    \textbf{75 \%} & 4.00 & 0.00 & 0.00 & 0.00\\
%    \hline
%    \textbf{max} & 3749.00 & 271.00 & 35.00 & 100.00\\
%    \hline     
%    \end{tabular}
%    \label{tab:addlabel}\\
%  {\scriptsize Fuente: Elaboración Propia}
%\end{table}

\begin{table}[H]
  \centering
  \textbf{\caption{{\small Estadísticos dataset covid-19.}}}
    \begin{tabular}{c c c c c}
    \midrule[0.7mm]
     & \makebox[2.2cm][c]{\textbf{num\_casos}} & \makebox[2.2cm][c]{\textbf{num\_hosp}} & \makebox[3cm][c]{\textbf{num\_uci}} &  \makebox[2.2cm][c]{\textbf{num\_def}}\\
    \midrule
    \textbf{count} & $1461210.00$ & $1461210.00$     & $1461210.00$ & $1461210.00$\\
    \textbf{mean} & $8.74$ &  $0.43$    & $0.04$ & $0.08$ \\
    \textbf{std} & $48.38$ & $2.49$    &  $0.30$ & $0.77$\\
    \textbf{min} & $0.00$ & $0.00$    &  $0.00$ & $0.00$\\
    \textbf{25\%} & $0.00$ & $0.00$    &  $0.00$ & $0.00$\\
    \textbf{50\%} & $0.00$ & $0.00$    &  $0.00$ & $0.00$\\
    \textbf{75\%} & $4.00$ & $0.00$    &  $0.00$ & $0.00$\\
    \textbf{max} & $3749.00$ & $271.00$    &  $35.00$ & $100.00$\\
    \bottomrule[0.7mm]
    \end{tabular}%
  \label{tab:addlabel}\\
  \medskip 
{\scriptsize Fuente: Elaboración Propia}
\end{table}%

La media de \textit{num\_casos} es de 8.74 y la mediana es 0 ya que en muchas fechas diarias no se reportaron casos, la media de \textit{num\_hosp} es de 0.43 y la mediana 0, la media de \textit{num\_uci} es 0.04 y la mediana 0, por último, la media de \textit{num\_def} es de 0.08 y la mediana 0. Las variables parecen ser asimétricas a la derecha dado que su media es mayor a su mediana. 

Las desviaciones típicas son bajas, la mayoría de las observaciones se encuentran dispersas a no más de una desviación estándar a cada lado.
La imagen anterior también muestra los valores máximos y mínimos que toman cada variable objeto de estudio además de los cuartiles calculados. Los datos menores al cuartil 1 (Q1) representan el 25\% de los datos, los que están por debajo del cuartil 2 (Q2) representan el 50\% de los datos y los que están por debajo del cuartil 3 (Q3) representan el 75\% de los datos.

Tras realizar un análisis de correlación entre sus variables de estudio del conjunto de datos se evidencia en la siguiente gráfica que las variables \textit{num\_hosp}, \textit{num\_uci} y \textit{num\_def} tiene una correlación positiva y fuerte entre ellas. Es de esperarse este resultado ya por lo general son consecuentes una con la otra, es decir, por ejemplo, si hubo una difusión por covid-19 es muy probable que haya estado en uci y hospitalización previamente.
\begin{figure}[H]
	\begin{center}
	\includegraphics[width=0.8\textwidth]{correla_covid}
		\textbf{\caption{{\small Correlación variables dataset covid-19. \label{tb:correla_covid}}}}
	{\scriptsize Fuente: [Elaboración propia]}
	\end{center}
\end{figure}

Se realizo un análisis de outliers para cada una de las variables, así como la distribución de los valores de las variables categóricas para identificar valores atípicos, para ningún caso se haya evidencia de alguno. Para evitar que existan palabras distintas y que simbolicen el mismo significado solo por el hecho de estar en minúscula o mayúsculas, para todas las variables tipo string las pasaremos a mayúscula, ya que por defecto todas viene así, también eliminaremos los espacios al principio y al final.

\item \textbf{Datos códigos provincias}
\\\\
Esta fuente de datos fue de elaboración propia por lo cual se aseguro que no hubiera valores duplicado, valores nulos, valores atípicos, el nombre de las columnas está en mayúscula, los valores están en mayúscula sin ningún tipo de espacio por ende no necesita ningún tipo de transformación. Esta fuente cuenta con dos columnas y con 52 registros, el cual servirá para unir los datasets.

\item \textbf{Datos climatológicos por provincia}
\\\\
Se realiza la concatenación de cada uno de los archivos con la información climatológica por provincia para poder obtener un dataset completo y su respectivo análisis, comenzando por la cantidad de nulos en todas sus variables mediante un heatmap.
\newpage
\begin{figure}[H]
	\begin{center}
	\includegraphics[width=0.8\textwidth]{heatmap_clima}
		\textbf{\caption{{\small Headmap variables dataset clima.}}}
	{\scriptsize Fuente: [Elaboración propia]}
	\end{center}
\end{figure}

El gráfico anterior muestra los patrones de datos que faltan de todas las columnas, el eje horizontal muestra el nombre del atributo de entrada; el eje vertical muestra el número de observaciones/filas; el color amarillo representa los datos que faltan, mientras que el color azul, en caso contrario. Detallamos que todas las características tienen muy pocos valores perdidos o inclusive no tienen, para tener un valor exacto hacemos una lista de porcentaje de valores nulos:
\begin{figure}[H]
	\begin{center}
	\includegraphics[width=0.25\textwidth]{list_nulos_clima}
		\textbf{\caption{{\small Porcentaje de nulos dataset clima.}}}
	{\scriptsize Fuente: [Elaboración propia]}
	\end{center}
\end{figure}
La imputación de los valores faltantes de las variables se estableció mediante \href{https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.fillna.html}{fillna} aplicado al dataframe por el método \textit{ffill} como primera opción, como segunda opción el método \textit{bfill}. El primer método usa la anterior observación válida para llenar el valor faltante y el segundo método usa la siguiente observación válida para llenar el valor faltante. La elección de estos métodos es debido a que son variables climatológicas, en donde el clima entre estaciones climáticas y periodos de tiempo corto son similares, la granularidad de este set de datos es diaria por lo que la imputación de valores faltantes se hará sobre el valor del día anterior o el más próximo.
Se elimina las variables indicativo y nombre, ya que estás hacen referencia netamente a la información de la estación meteorológica en donde se obtuvieron los datos, esta información no aporta a nuestro estudio.

Tras realizar un análisis de correlación entre sus variables de estudio del conjunto de datos se evidencia en la siguiente gráfica que las variables \textit{PREX\_MAX} y \textit{PREX\_MIN} tiene una correlación positiva y fuerte entre ellas; así como las variables \textit{TEMP\_MIN}, \textit{TEMP\_MAX} y \textit{TEMP\_MED} tienen una correlación positiva alta. 
\begin{figure}[H]
	\begin{center}
	\includegraphics[width=0.85\textwidth]{correla_clima}
		\textbf{\caption{{\small Correlación variables dataset clima. \label{tb:correla_clima}}}}
	{\scriptsize Fuente: [Elaboración propia]}
	\end{center}
\end{figure}
Se realizo un análisis de outliers por medio de boxplots para cada una de las variables, así como la distribución de los valores de las variables categóricas para identificar valores atípicos, se encontraron valores alejados de la media y poco comunes, pero son valores climatológicos posibles por esta razón para ningún caso se haya evidencia de outliers. Para evitar que existan palabras distintas y que simbolicen el mismo significado solo por el hecho de estar en minúscula o mayúsculas, para todas las variables tipo string las pasaremos a mayúscula, ya que por defecto todas viene así, también eliminaremos los espacios al principio y al final.
\item \textbf{Datos población por provincia}
\\\\
Para este set de datos se eliminó las variables Sexo, Edad (año a año) y Españoles/Extranjeros, ya que solo nos interesa la población anual por provincia para poder generar la tasa de incidencia de covid-19 mensual. Como primer paso se crea una columna equivalente a la tasa de crecimiento mensual ya que tenemos la población anual, se utiliza una de las formulas más utilizadas para cálculos poblacionales como lo es el modelo geométrico [14]. A continuación, se describe la fórmula utilizada:
\begin{equation}
r = \left(\frac{P_f}{P_i}\right)^\frac{1}{t}-1
\end{equation}
donde:
\\\\
$r$ \hspace{1em} Tasa de crecimiento mensual\\
$P_f$ \hspace{0.4em} Población final\\
$P_i$ \hspace{0.52em} Población inicial\\
$t$ \hspace{1em} Distancia en tiempo entre las dos poblaciones de referencia
\\\\
Tomaremos la población inicial del año 2019 y la población actual del año 2022, esto debido a que en los años anteriores en casi todos los casos aumento la población, pero en este periodo de tiempo el comportamiento fue diferente a razón del covid-19, el cálculo de la tasa de crecimiento mensual se utilizará para calcular el aproximado de la población mensual por provincia hasta el primer trimestre del 2023. Se realiza el calculo de la población mensual con proyección en un periodo t, mediante la siguiente ecuación:
\begin{equation}
P_f = P_i(1 + r)^t 
\end{equation}
donde:
\\\\
$P_f$ \hspace{0.4em} Población final en ese caso mes a mes\\
$P_i$ \hspace{0.5em} Población Inicial en este caso del comienzo de cada año\\
$r$ \hspace{0.8em} Tasa de crecimiento mensual calculada anteriormente\\
$t$ \hspace{1.1em} La proyección en tiempo, en este caso el mes a calcular
\\\\
El valor de esta población mensual tomara el nombre de \textit{POB\_MEN}
\item \textbf{Dataset total}
\\\\
Este dataset está conformado por el dataset climatológico unido a la fuente de datos cod\_iso\_provincias por medio del campo \textit{PROVINCIA} esto con el fin de añadir la columna \textit{PROVINCIA\_ISO}, a su vez este dataset se unirá a la fuente de datos de covid por medio de la columna \textit{PROVINCIA\_ISO} y la \textit{FECHA}, para generar el dataset total, este proceso se describe mediante los siguientes comandos en Python:
\\
\begin{lstlisting}[style=Python]
df_clima_iso = df_clima.merge(df_iso, how="inner", on="PROVINCIA")
df_total = df_covid.merge(df_clima_iso, how="inner", 
			on=["FECHA", "PROVINCIA_ISO"])
\end{lstlisting}

A este dataset total se eliminaron las variables de Horas (\textit{HORA\_TEMP\_MIN, HORA\_TEMP\_MAX, HORA\_RACHA, HORA\_PRES\_MAX, HORA\_PRES\_MIN}) ya que no deseamos un nivel de granularidad tan bajo, por el contrario, se tomara en cuenta la demás variables climatológicas diarias; se elimina la variable \textit{PROVINCIA\_ISO} ya que tenemos la variable \textit{PROVINCIA} la cual hace referencia al mismo significado; se elimina las variables \textit{NUM\_HOSP, NUM\_UCI, NUM\_DEFU} esto debido a la explicación de la figura \textbf{\ref{tb:correla_covid}} y el objetivo principal del estudio es la propagación del virus covid-19 es decir el número de casos (\textit{NUM\_CASOS}) y no las defunciones y/o hospitalizaciones; se elimina las variables \textit{TEMP\_MIN, TEMP\_MAX} esto debido a la explicación de la figura \textbf{\ref{tb:correla_clima}} y se conserva la variable \textit{TEMP\_MED}; por el momento se eliminan las variables \textit{GRUPO\_EDAD, SEXO} para centrar el estudio en la propagación del virus entorno a las variables climatológicas. Tras esta serie de pasos se realiza una gráfica de tendencia de los casos de covid de todas las provincias para tener un panorama más amplio del caso de estudio.
\begin{figure}[H]
	\begin{center}
	\includegraphics[width=0.80\textwidth]{tendencia_casos_covid}
		\textbf{\caption{{\small Tendencia de casos covid. \label{tb:tendencia_casos}}}}
	{\scriptsize Fuente: [Elaboración propia]}
	\end{center}
\end{figure}
Se crea la variable \textit{TASA\_INCIDENCIA} a partir de la normalización del número de casos, dada por la siguiente formula:
\begin{equation}
TASA\_INCIDENCIA = \left(\frac{NUM\_CASOS}{POB\_MEN}\right) \times 100000
\end{equation}
En donde \textit{POB\_MEN} (población mensual) se calculó en el ítem anterior para cada año desde el 2020 hasta el primer trimestre del 2023 por provincia. De igual forma se realiza una gráfica de tendencia de la tasa de incidencia de todas las provincias, como se puede observar en la siguiente figura la tasa de incidencia es una muy buena normalización con respecto a los casos covid de la figura \textbf{\ref{tb:tendencia_casos}}.
\begin{figure}[H]
	\begin{center}
	\includegraphics[width=0.80\textwidth]{tendencia_tasa_incidencia}
		\textbf{\caption{{\small Tendencia tasa de incidencia. \label{tb:tendencia_tasa_incid}}}}
	{\scriptsize Fuente: [Elaboración propia]}
	\end{center}
\end{figure}
\end{itemize}

\subsubsection{TRANSFORMACIÓN DE DATOS}
Para cada una de las fuentes de datos descrita anteriormente se realiza la transformación de su información, así como el dataset total que esta compuesto de la unión de estás fuentes.
\begin{itemize}
\item \textbf{Datos casos COVID-19 por provincias}
\\\\
Se realizo la transformación del nombre de todas las columnas a mayúscula, de esta forma se trabajará en todos los dataset para manejar un estándar de nombramiento, se realiza conversión de la variable de tiempo \textit{fecha} a tipo ``datetime'', las variables que estén tipo float y no tengan ningún valor decimal se convertirán en enteros. Normalización y homologación de los valores en campos categóricos con el fin de agrupar y estandarizar, obteniendo así los siguientes tipos de datos:
\begin{figure}[H]
	\begin{center}
	\includegraphics[width=0.38\textwidth]{tipo_datos_covid}
		\textbf{\caption{{\small Tipos de datos dataset covid-19.}}}
	{\scriptsize Fuente: [Elaboración propia]}
	\end{center}
\end{figure}

\item \textbf{Datos climatológicos por provincia}
\\\\
Se realizo la transformación del nombre de todas las columnas a mayúscula, la conversión de la variable de tiempo \textit{fecha} a tipo ``datetime'', las variables que estén tipo float y no tengan ningún valor decimal se convertirán en enteros, así como las variables que son de tipo string y que en realidad todos son datos son numéricos decimales se realizará su correspondiente transformación a tipo float. A la variable prec (Precipitación diaria de 07 a 07) se transformó el valor `Ip' (significa precipitación inapreciable, es decir, cantidad inferior a 0.1 mm) por 0,0.

Para todas las variables de horas se transformo el valor ``24'' por ``00'' ya que hacen referencia a la misma hora, más adelante se explicará porque estas variables de horas no serán tomadas en cuenta para la etapa de minería de datos (creación de modelos). Inicialmente solo dos campos eran numéricos, tras realizar el proceso de transformación obtenemos los siguientes tipos de datos:
\begin{figure}[H]
	\begin{center}
	\includegraphics[width=0.38\textwidth]{tipo_datos_clima}
		\textbf{\caption{{\small Tipos de datos dataset clima.}}}
	{\scriptsize Fuente: [Elaboración propia]}
	\end{center}
\end{figure}

\item \textbf{Datos población por provincia}
\\\\
Se transforma la variable \textit{Provincias} y se hace la homologación con los mismos nombres de las provincias como en los demás conjuntos de datos, es decir, se reemplazan algunos nombres; la variable \textit{Total} se transforma a entero ya que todos sus datos tienen decimales con ceros, además de que este debe ser un valor entero; una vez calculada la población mensual en el preprocesamiento se eliminan las variables intermedias y que no son de utilidad como \textit{YEAR, YEAR\_ACUM, TOTAL\_POB, TASA\_MENSUAL}

\item \textbf{Dataset total}
\\\\
Se realiza la transformación de la variable \textit{FECHA} cambiando su granularidad de diaria a mensual, de esta forma se procede a hacer la agrupación de los datos por \textit{FECHA} y \textit{PROVINCIA} y todas las demás medidas se le hace un promedio excepto la variable \textit{NUM\_CASOS} que será la suma de todos los días para el mes correspondiente. Se eliminan las variables \textit{NUM\_CASOS, POB\_MEN} tras el calculo de la tasa de incidencia de covid-19 en el ítem anterior, obteniendo así el dataset final con los siguientes tipos de datos:
\begin{figure}[H]
	\begin{center}
	\includegraphics[width=0.38\textwidth]{tipo_datos_final}
		\textbf{\caption{{\small Tipos de datos dataset total.}}}
	{\scriptsize Fuente: [Elaboración propia]}
	\end{center}
\end{figure}
%Realizando un análisis descriptivo de nuestro dataset final, se observa que las desviaciones típicas son bajas a excepción de la \textit{ALTITUD} y \textit{TASA\_INCIDENCIA}
Este dataset final será exportado como archivo data\_refined.csv y será tomado como partida de referencia para la construcción de los modelos en nuestra etapa de minería de datos.

\end{itemize}

\subsubsection{MINERÍA DE DATOS}
En esta etapa de nuestro proceso de KDD partiremos de nuestro dataset final (data\_refined.csv) en donde descubriremos patrones y conocimiento mediante técnicas y algoritmos de machine learning, a continuación, se describen algunos de estos y su proceso:

\subsubsubsection{REGRESIÓN LINEAL MÚLTIPLE}
\\
Para este modelo la variable dependiente será la TASA\_INCIDENCIA que será la variable que queremos predecir las demás variables de nuestro dataset final serán las variables independientes. En este caso también tenemos una variable categórica \textit{PROVINCIA}. Esta variable es transformada mediante el método de codificación hash. Se realizaron pruebas con la codificación One-Hot encoding obteniendo resultados muy similares o un poco menores con respecto a las métricas de medida, esto debido a que la principal debilidad de One-Hot encoding es que las características que produce son equivalentes al cardinal categórico, lo que causa problemas de dimensionalidad cuando la cardinalidad es demasiado alta, por el contrario la codificación hash representa los datos categóricos en valor numérico mediante la función hash. \\\\
La principal ventaja de usar Hash Encoding es que puede controlar la cantidad de columnas numéricas producidas por el proceso, debemos tener en cuenta que Hashing transforma los datos en dimensiones menores, puede provocar la pérdida de información y una gran cantidad de características que se representan en dimensiones menores pueden representar múltiples valores con el mismo valor hash, esto se conoce como colisión. Es por esto que se debe escoger un valor optimo de columnas numéricas que representaran la variable. En general es ideal cuando el conjunto de datos tiene características de alta cardinalidad, para nuestro caso la variable \textit{PROVINCIA} tiene 52 valores. A su vez se crea la variable \textit{MONTH} a partir de la fecha, se tendrán en cuenta todas las variables excepto la variable \textit{FECHA}.
La ecuación del modelo quedaría de la siguiente forma:
\begin{align*}
TASA\_INCIDENCIA = b_o + b_1 MONTH + b_2 ALTITUD + b_3 TEMP_MED \\
+ b_4 PREC + b_5 DIR + b_6 VEL_MEDIA + b_7 RACHA + b_8 PRES_MIN + b_9 SOL \\
+ b_{10} HASH_0 + b_{11} HASH_1 + b_{12} HASH_2 + b_{13} HASH_3 + b_{14} HASH_4 + b_{15} HASH_5
\end{align*}

Dividimos nuestro dataset total en train y test de forma aleatoria en una relación de 70\% a 30\% respectivamente, esta división se realiza de forma aleatoria ya que existe un pico de casos en el mes de enero del 2021 y mediante pruebas se evidencia que al tomar el test a partir de una fecha fija el modelo no es tan preciso ya que se entrena con un comportamiento totalmente diferente (comienzos de covid) contra el test (finales de covid).  Creamos el modelo a partir de linear\_model.LinearRegression(), al realizar el entrenamiento, obtenemos los siguientes coeficientes y estadísticos para la ecuación del modelo:
\newpage
\begin{figure}[H]
	\begin{center}
	\includegraphics[width=1\textwidth]{summary_RL}
		\textbf{\caption{{\small Estadisticos modelo regresión lineal múltiple. \label{tb:summary_RL}}}}
	{\scriptsize Fuente: [Elaboración propia]}
	\end{center}
\end{figure}

Tomando los coeficientes de cada una de las variables, nuestra ecuación final queda de la siguiente forma:
\begin{align*}
TASA\_INCIDENCIA = -102000 + 18.88 \cdot MONTH + 11.21 \cdot ALTITUD \\
- 87.23 \cdot TEMP\_MED + 26.54 \cdot PREC + 0.18 \cdot DIR + 49.05 \cdot VEL\_MEDIA \\
- 50.32 \cdot RACHA + 101.83 \cdot PRES\_MIN + 131.18 \cdot SOL - 2.16 \cdot HASH\_0 \\ 
+ 0.91 \cdot HASH\_1 + 6.95 \cdot HASH\_2 + 6.59 \cdot HASH\_3 + 10.94 \cdot HASH\_4 \\
- 26.81 \cdot HASH\_5
\end{align*}
La constante $b_o$ es -10200 es decir nuestra intercepción, los demás coeficientes $b_i$ son los valores numéricos que acompañan a cada una de las variables independientes de nuestro modelo, la interpretación del coeficiente $b_1$ es que por cada unidad que se incrementa el \textit{MES} el incremento de la tasa de incidencia será de $18.88$ unidades; el coeficiente $b_2$ por cada unidad que se incremente la \textit{ALTITUD} el incremento de la tasa de incidencia será de $11.21$ unidades; el coeficiente $b_3$ por cada unidad que se incremente la \textit{TEMP\_MED} hay un decrecimiento de la tasa de incidencia de $-87.23$ unidades; de igual forma sucesivamente es la interpretación de los demás coeficientes.\\\\
De la \textbf{figura \ref{tb:summary_RL}} concluimos que todas las variables introducidas como predictores tienen un $R^2$ bajo (0.152) es decir es capaz de explicar en un 15.2\% la variabilidad observada de la tasa de incidencia, acorde al p-value obtenido para el coeficiente parcial de regresión para algunas variables de HASH, VEL\_MEDIA, DIR y PREC, estas variables no contribuyen de forma significativa el modelo. Se realizo el entrenamiento sin estás variables y se obtuvieron resultados muy similares, se dejo este modelo para evidenciar el impacto de estas variables en los resultados.
Los intervalos de confianza obtenidos son:
\begin{table}[H]
  \centering
   \textbf{\caption{{\small Intervalos de confianza. \label{tb:TableintervaRL}}}}       
	\renewcommand{\arraystretch}{1.2}  
  %{\setlength{\arrayrulewidth}{0.3mm}
    \begin{tabular}{|c|c|c|}
    \hline
    \rowcolor{Peach}  & 2.5\% & 97.5\%\\
	\hline    
    CONST & -127300.37 & -76677.83 \\
    \hline
    MONTH & -5.179000 & 42.950290 \\
    \hline 
    ALTITUD & 8.417721 & 14.019315 \\
	\hline
    TEMP\_MED & -108.903632 & -65.569304 \\
    \hline
    PREC & -21.210277 & 74.293363 \\
    \hline
    DIR & -6.245870 & 6.607292 \\
    \hline
    VEL\_MEDIA & -83.375974 & 181.481665 \\
	\hline
	RACHA & -134.093622 & 33.440448 \\
	\hline
	PRES\_MIN & 77.195649 & 126.475020 \\
	\hline
	SOL & 82.268584 & 180.102275 \\
	\hline
	HASH\_0 & -101.736920 & 97.411155 \\
	\hline
	HASH\_1 & -128.324197 & 130.150506 \\
	\hline
	HASH\_2 & -79.407085 & 93.326155 \\
	\hline
	HASH\_3 & -42.361196 & 55.556558 \\
	\hline
	HASH\_4 & -39.404144 & 61.289839 \\
	\hline
	HASH\_5 & -127.819261 & 74.179561 \\
	\hline   
    \end{tabular}
    \label{tab:addlabel}\\
  {\scriptsize Fuente: Elaboración Propia}
\end{table}

Los residuos del modelo muestra que hay una alta acumulación sobre la línea recta del cero, es decir el promedio de los errores es cero; los residuos Q-Q con respecto se acerquen más los puntos a la recta mayor normalidad de los residuos, pero hay una gran cantidad de valores por debajo de -2, la pendiente de la recta está muy arriba y no logran estar todos los valores sobre la diagonal, porque hay unos datos que están por debajo de la distribución, esto se evidencia en el gráfico de distribución que esta sesgado a la derecha, el p-valor es menor al valor de significancia (5\%), por tanto no se satisface la hipótesis, es decir, los residuos no satisfacen una distribución normal. \\
Los diagnósticos de los residuos fueron los siguientes:
\newpage
\begin{figure}[H]
	\begin{center}
	\includegraphics[width=1\textwidth]{residuos_RL}
		\textbf{\caption{{\small Diagnóstico residuos modelo regresión lineal múltiple. \label{tb:residuos_RL}}}}
	{\scriptsize Fuente: [Elaboración propia]}
	\end{center}
\end{figure}

Para el test de normalidad se obtuvo:
\begin{table}[H]
  \centering
   \textbf{\caption{{\small Test estadísticos para normalidad. \label{tb:TableintervaRL}}}}       
	\renewcommand{\arraystretch}{1.2}  
  %{\setlength{\arrayrulewidth}{0.3mm}
    \begin{tabular}{|c|c|c|}
    \hline
    \rowcolor{Peach} Test Estadístico & Estadístico & p-value\\
	\hline    
    Shapiro-Wilk & 0.7023 & 4.6242e-44 \\
    \hline
    D'Agostino's K-squared  & 1043.6025 & 2.4243e-227 \\
    \hline       
    \end{tabular}
    \label{tab:addlabel}\\
  {\scriptsize Fuente: Elaboración Propia}
\end{table}
Es decir, se comprueba si los residuos siguen una distribución normal empleando dos test estadísticos: Shapiro-Wilk test y D'Agostino's K-squared test. Este último es el que incluye el summary de statsmodels bajo el nombre de Omnibus de la gráfica summary.
En ambos test, la hipótesis nula considera que los datos siguen una distribución normal, por lo tanto, si el p-value no es inferior al nivel de referencia alpha seleccionado, no hay evidencias para descartar que los datos se distribuyen de forma normal y Ambos test muestran claras evidencias para rechazar la hipótesis de que los datos se distribuyen de forma normal (p-value $\ll$ 0.01). Las métricas para la evaluación del modelo se mostrara en el apartado de resultados.

\subsubsubsection{RANDOM FOREST REGRESIÓN}
\\
De igual forma que en el modelo de regresión Lineal múltiple la variable dependiente será la \textit{TASA\_INCIDENCIA} que será la variable que queremos predecir las demás variables de nuestro dataset final serán las variables independientes. La variable categórica \textit{PROVINCIA}, fue transformada mediante el método de codificación hash. Se realizaron pruebas con la codificación One-Hot encoding obteniendo resultados muy similares o un poco menores con respecto a las métricas de medida. A su vez se crea la variable \textit{MONTH} a partir de la fecha, se tendrán en cuenta todas las variables excepto la variable \textit{FECHA}. Se divide el dataset total en train y test de forma aleatoria en una relación de 70\% a 30\% respectivamente, esta división se realiza de acuerdo a la explicación del modelo de regresión lineal.
\\\\
Para la creación de este modelo se utilizó RandomForestRegressor de la librería sklearn ensemble, los hiperparámetros por defecto son:
\begin{itemize}
\item \textbf{n\_estimadores:} El número de árboles en el bosque. Int, por defecto = 100.
\item \textbf{criterion:}  La función para medir la calidad de una división. Los criterios admitidos son ``squared\_error'' para el error cuadrático medio, que es igual a la reducción de la varianza como criterio de selección de características y minimiza la pérdida de L2 utilizando la media de cada nodo terminal, ``friedman\_mse'', que utiliza el error cuadrático medio con la puntuación de mejora de Friedman para el potencial splits, ``absolute\_error'' para el error absoluto medio, que minimiza la pérdida L1 usando la mediana de cada nodo terminal, y ``poisson'' que usa la reducción en la desviación de Poisson para encontrar divisiones. El entrenamiento con ``absolute\_error'' es significativamente más lento que cuando se usa ``squared\_error'', default = ``squared\_error''.
\item \textbf{max\_depth:} La profundidad máxima del árbol. Si es Ninguno, los nodos se expanden hasta que todas las hojas sean puras o hasta que todas las hojas contengan menos de min\_samples\_split samples. Int, por defecto = None.
\item \textbf{min\_samples\_split:} El número mínimo de muestras requeridas para dividir un nodo interno. Int o float, por defecto = 2.
\item \textbf{min\_samples\_leaf:} El número mínimo de muestras requeridas para estar en un nodo hoja. Un punto de división a cualquier profundidad solo se considerará si deja al menos min\_samples\_leaf muestras de entrenamiento en cada una de las ramas izquierda y derecha. Esto puede tener el efecto de suavizar el modelo, especialmente en regresión. Int o float, por defecto = 1.
\item \textbf{min\_weight\_fraction\_leaf:} La fracción ponderada mínima de la suma total de pesos (de todas las muestras de entrada) requerida para estar en un nodo hoja. Las muestras tienen el mismo peso cuando no se proporciona sample\_weight. Float, por defecto=0.0
\item \textbf{max\_features:} El número de características a considerar al buscar la mejor división. (``sqrt'', ``log2'', None), int o float, predeterminado=1.0
\item \textbf{max\_leaf\_nodes:}  Cultiva árboles max\_leaf\_nodesde la mejor manera. Los mejores nodos se definen como una reducción relativa de la impureza. Si es Ninguno, entonces un número ilimitado de nodos hoja. Int, predeterminado = None.
\item \textbf{min\_impurity\_decrease:} Un nodo se dividirá si esta división induce una disminución de la impureza mayor o igual a este valor. Float, predeterminado = 0.0
\item \textbf{bootstrap:} Si se utilizan muestras de arranque al construir árboles. Si es False, se usa todo el conjunto de datos para construir cada árbol. Predeterminado = True.
\item \textbf{n\_jobs:} El número de trabajos a ejecutar en paralelo. Int, predeterminado = None.
\item \textbf{random\_state:} Controla tanto la aleatoriedad del arranque de las muestras utilizadas al construir árboles (si bootstrap = True) como el muestreo de las características a considerar cuando se busca la mejor división en cada nodo. Int, instancia de RandomState o None, predeterminado = None.
\item \textbf{verbose:} Controla la verbosidad al ajustar y predecir. Int, predeterminado = 0.
\item \textbf{warm\_start:} Cuando se establece en True, reutiliza la solución de la llamada anterior para ajustar y agregar más estimadores al conjunto; de lo contrario, solo ajusta un bosque completamente nuevo. Booleano, predeterminado = False.
\item \textbf{ccp\_alpha:} Parámetro de complejidad utilizado para la poda de complejidad de costo mínimo. Float no negativo, por defecto = 0.0
\item \textbf{max\_samples:} Si bootstrap es True, el número de muestras que se extraerán de X para entrenar cada estimador base. Int o float, predeterminado = None.
\end{itemize}

El modelo creado utilizo todos los parámetros anteriores por defecto a excepción de \textit{criterion}, \textit{n\_estimators} y \textit{random\_state}; los cuales se utilizaron los valores de absolute\_error para \textit{criterion}, un rango de números enteros (2, 4, 8, 16, 32, 64, 128, 256) para los \textit{n\_estimators} y 0 para \textit{random\_state}. No se realiza el gráfico del árbol de decisión ya que esta es una propiedad de los clasificadores y no para los regresores. A partir de nuestro dataset de train se evaluó el MAE para cada uno de los valores de los \textit{n\_estimators}, esta representación gráfica se muestra a continuación:
\begin{figure}[H]
	\begin{center}
	\includegraphics[width=0.6\textwidth]{mae_random_forest}
		\textbf{\caption{{\small MAE para cada estimador del modelo Random Forest Regresión. \label{tb:mae_random_forest}}}}
	{\scriptsize Fuente: [Elaboración propia]}
	\end{center}
\end{figure}

Se evidencia que el MAE decrece conforme avanza el valor de n\_estimators, pero a partir de 64 casi que el valor del MAE permanece constante, al realizar la búsqueda del mínimo MAE se encuentra con n\_estimators = 256, este valor será el utilizado para entrenar y ajustar el modelo.

\subsubsubsection{SERIE TEMPORAL FORECASTERAUTOREG}
\\
Nuestra variable dependiente será la \textit{TASA\_INCIDENCIA} que será la variable que queremos predecir las demás variables de nuestro dataset final serán las variables independientes. En comparación a los dos modelos anteriores se divide nuestro dataset de train y test en un rango de fecha tomando el set de train desde 2020-01 hasta 2022-02-01, y el dataset de test será desde 2022-03 hasta 2023-02. Se creo un modelo de serie temporal a nivel nacional (España). Nuestra partición de datos tiene la siguiente forma como se muestra a continuación:
\begin{figure}[H]
	\begin{center}
	\includegraphics[width=1\textwidth]{train_test_series}
		\textbf{\caption{{\small División train y test ForecasterAutoreg. \label{tb:train_test_series}}}}
	{\scriptsize Fuente: [Elaboración propia]}
	\end{center}
\end{figure}

Para la creación de este modelo se utilizó ForecasterAutoreg de la librería skforecast, este modelo convierte cualquier regresor compatible con la API scikit-learn en un pronosticador recursivo autorregresivo (de varios pasos) y cuenta con los siguientes hiperparámetros:
\begin{itemize}
\item \textbf{regressor:} Una instancia de un regresor compatible con la API de scikit-learn.
\item \textbf{lags(int, list, 1D np.array, range):} retrasos utilizados como predictores. El índice comienza en 1, por lo que el retraso 1 es igual a t-1. int: incluye retardos de 1 a lags (incluido). List o np.array: incluir solo los retrasos presentes en lags.
\end{itemize}

El modelo creado utilizo como regressor un RandomForestRegressor (con los mismos parámetros descritos en el modelo anterior esto con el fin de contrastar ambos modelos bajo las mismas condiciones) y lags = 10.
\\\\
La descomposición de nuestra serie temporal nos muestra que a lo largo del tiempo no hay una tendencia clara de la tasa de incidencia de covid-19; existe una estacionalidad con picos en enero en todos los años, esto en parte es cierta debido a fenómenos sociales de fin de año y que el incremento de casos de covid se vea reflejado en el mes de enero, pero estos picos no son de la misma magnitud en los eneros de cada año, esto no se evidencia en la estacionalidad de la serie temporal; el residuo o componente aleatorio (que no pudo ser explicado por la tendencia o estacionalidad) es bastante atípico o invariante, esto quiere decir que muy posiblemente nuestro modelo no pueda predecir de forma correcta las tasas de incidencia futuras y que nuestras métricas de evaluación no van a ser las mejores, en el apartado de resultados se observaran estas métricas. Los componentes descritos se muestran a continuación:
\begin{figure}[H]
	\begin{center}
	\includegraphics[width=1\textwidth]{descomp_series}
		\textbf{\caption{{\small Descomposición serie temporal ForecasterAutoreg. \label{tb:descomp_series}}}}
	{\scriptsize Fuente: [Elaboración propia]}
	\end{center}
\end{figure}

\subsubsubsection{K-MEANS}
\\
Este es un tipo de modelo no supervisado, es decir, no vamos a tener una etiqueta para predecir y aunque la tenemos no la vamos a utilizar, es decir, la tasa de incidencia no será una etiqueta a predecir, por el contrario, será una variable más en nuestro modelo, esto para generar clúster o una clasificación de los datos, con el fin de encontrar patrones de comportamiento. La variable \textit{FECHA} será eliminada ya que como patrón no nos interesa, y la variable \textit{PROVINCIA} será transformada como index en nuestro dataset.
\\\\
Para la creación de este modelo se utilizó KMeans de la librería sklearn clúster, los hiperparámetros por defecto son:

\begin{itemize}
\item \textbf{n\_clusters:} El número de clústeres a formar, así como el número de centroides a generar. Int, por defecto = 8.
\item \textbf{init:} Método de inicialización, ``k-means++'': selecciona los centroides de clúster iniciales utilizando un muestreo basado en una distribución de probabilidad empírica de la contribución de los puntos a la inercia general. Esta técnica acelera la convergencia. El algoritmo implementado es ``greedy k-means++''. Se diferencia de k-means++ en que realiza varios ensayos en cada paso de muestreo y elige el mejor centroide entre ellos. Por defecto = ``k-means++''.
\item \textbf{n\_init:} Número de veces que se ejecuta el algoritmo k-means con semillas de centroide diferentes. El resultado final es el mejor resultado de n\_init ejecuciones consecutivas en términos de inercia. Por defecto = 10.
\item \textbf{max\_iter:}  Número máximo de iteraciones del algoritmo k-means para una sola ejecución. Int, por defecto = 300.
\item \textbf{tol:} Tolerancia relativa respecto a la norma de Frobenius de la diferencia en los centros de los clusters de dos iteraciones consecutivas para declarar convergencia. Float, por defecto=1e-4.
\item \textbf{verbose:} Modo de verbosidad. int, por defecto=0.
\item \textbf{random\_state:} Determina la generación de números aleatorios para la inicialización del centroide. Use un int para hacer que la aleatoriedad sea determinista. Int, RandomState instance o None, por defecto = None.
\item \textbf{copy\_x:} Cuando se calculan previamente las distancias, es más preciso desde el punto de vista numérico centrar los datos primero. Si copy\_x es True (predeterminado), los datos originales no se modifican. Si es Falso, los datos originales se modifican y se vuelven a colocar antes de que regrese la función, pero se pueden introducir pequeñas diferencias numéricas restando y luego sumando la media de los datos. Bool, default = True.
\item \textbf{algorithm:} Algoritmo K-medias a utilizar. El algoritmo clásico de estilo EM es ``lloyd''. La ``elkan'' variación puede ser más eficiente en algunos conjuntos de datos con grupos bien definidos, mediante el uso de la desigualdad triangular. Sin embargo, requiere más memoria debido a la asignación de una matriz adicional de forma. {``lloyd'', ``elkan'', ``automático'', ``full''}, predeterminado = ``lloyd''.
\end{itemize}
El modelo creado utilizo los parámetros anteriores por defecto a excepción de n\_clusters y random\_state, para n\_cluster se utilizo un rango de valores de 1 a 10 para realizar la gráfica del codo y validar su inercia respecto al modelo, con respecto a random\_state se tomo un valor de 42, se utilizó un valor int para que la aleatoriedad sea determinista y evitar distintos valores en cada iteración de nuestro valor de n\_cluster. La representación de la gráfica del codo se muestra a continuación:
\newpage
\begin{figure}[H]
	\begin{center}
	\includegraphics[width=0.8\textwidth]{codo_kmeans}
		\textbf{\caption{{\small Método del codo K-means. \label{tb:codo_kmeans}}}}
	{\scriptsize Fuente: [Elaboración propia]}
	\end{center}
\end{figure}

Basado en la figura anterior, se debe tomar un n\_cluster igual a 3 o 4, que es donde comienza la curvatura del codo, se realizaron pruebas con 4 clúster y se evidencio que dos de estos clúster tenían características muy similares, por tal razón el valor optimo de n\_cluster será tomado como 3. Nuestro modelo se entrena y se ajusta bajo el parámetro anterior y se le asigna el clúster correspondiente a cada uno de los registros de nuestro dataset.

\subsection{RESULTADOS}
Para cada uno de los modelos descritos anteriormente se mostrará su gráfica de predicción versus los valores reales, así como las mismas métricas de evaluación ($R^2$, MAE y RMSE) para cada modelo y al final mediante un apartado de comparación y contraste se evaluará cual es el modelo que mejor se ajusta a nuestros datos y los beneficios que este aporta.
\begin{itemize}
\item \textbf{REGRESIÓN LINEAL MÚLTIPLE}
\\\\
Realizando una comparación de los valores reales con los predichos por nuestro modelo, notamos la recta que creo el modelo se encuentra desfasada y desajustada al valor real de los datos, por la naturaleza de los datos se puede crear una mejor recta que explique el comportamiento de nuestra variable dependiente (TASA\_INCIDENCIA) según la siguiente imagen, pero bajo nuestras variables de estudio (climáticas) no se puede generar una mejor recta que explique la TASA\_INCIDENCIA, se debería añadir más variables con una mayor importancia que aporten a nuestro modelo.
\newpage
\begin{figure}[H]
	\begin{center}
	\includegraphics[width=0.9\textwidth]{real_vs_pred_RL}
		\textbf{\caption{{\small Valor predicho vs Valor real Modelo Regresión Lineal Múltiple. \label{tb:real_vs_pred_RL}}}}
	{\scriptsize Fuente: [Elaboración propia]}
	\end{center}
\end{figure}

Las métricas calculadas para la evaluación del modelo son:
\begin{itemize}
\item \textbf{$R^2 = 0.15$}
\item \textbf{$MAE = 756.23$}
\item \textbf{$RMSE = 1337.30$}
\end{itemize}
Se concluye que todas las variables introducidas como predictores tienen un $R^2$ bajo (0.15) es decir es capaz de explicar en un 15.2\% la variabilidad observada de la tasa de incidencia.
Las predicciones del modelo se alejan en promedio $756.23$ unidades de nuestro valor real esto es explicado por el MAE ya que calcula el error en la misma escala de los datos. Por el contrario, el valor RMSE de $13373.30$ que mide la diferencia promedio al cuadrado entre los valores estimados y el valor real, penaliza valores extremos o valores atípicos es por esta razón que es un valor más alto que el MAE, para ambos casos lo ideal es lo más cercano a cero y saber la naturaleza de los datos, para nuestro caso la variable dependiente TASA\_INCIDENCIA, está entre el rango de $0.0$ hasta $12025.3$, teniendo un promedio de $728.86$.
\\
\item \textbf{RANDOM FOREST REGRESIÓN}
\\\\
Realizando una comparación de los valores reales con los predichos por nuestro modelo de RandomForestRegressor, notamos que existen unos picos los cuales el modelo no puede predecir, esto es debido a la naturaleza de los datos ya que durante la pandemia hubo picos de infección los cuales hacen que sea distintos a sus homólogos en los mismos meses de diferente año. Estos picos se deben a factores sociales (eventos, movilidad, época decembrina, apertura de establecimientos, etc) los cuales tienen muy poca correlación con los factores climáticos. La comparación se muestra en el siguiente gráfico junto con el n\_estimators utilizado y el resultado del MAE al evaluar el modelo.
\begin{figure}[H]
	\begin{center}
	\includegraphics[width=0.9\textwidth]{real_vs_pred_random_forest}
		\textbf{\caption{{\small Valor predicho vs Valor real Modelo Random Forest regresión. \label{tb:real_vs_pred_random_forest}}}}
	{\scriptsize Fuente: [Elaboración propia]}
	\end{center}
\end{figure}

Las métricas calculadas para la evaluación del modelo son:
\begin{itemize}
\item \textbf{$R^2 = 0.338$}
\item \textbf{$MAE = 689.75$}
\item \textbf{$RMSE = 1169.12$}
\end{itemize}

Se concluye que todas las variables introducidas como predictores tienen un $R^2$ bajo (0.338) es decir es capaz de explicar en un 33.8\% la variabilidad observada de la tasa de incidencia.
Las predicciones del modelo se alejan en promedio $689.75$ unidades de nuestro valor real esto es explicado por el MAE ya que calcula el error en la misma escala de los datos. Por el contrario, el valor RMSE de $1169.12$.
\\\\
La importancia de las variables que utilizamos en nuestro modelo se muestra en la siguiente tabla:
\newpage
\begin{table}[H]
  \centering
   \textbf{\caption{{\small Importancia de las variables en el modelo. \label{tb:TableImportVarRF}}}}       
	\renewcommand{\arraystretch}{1.2}  
  %{\setlength{\arrayrulewidth}{0.3mm}
    \begin{tabular}{|c|c|}
    \hline
    \rowcolor{Peach} Variable & Importancia \\
	\hline        
    ALTITUD & 0,037044 \\
	\hline
    TEMP\_MED & 0,138512 \\
    \hline
    PREC & 0,112269 \\
    \hline
    DIR & 0,096510 \\
    \hline
    VEL\_MEDIA & 0,083530 \\
	\hline
	RACHA & 0,102015 \\
	\hline
	PRES\_MIN & 0,116705 \\
	\hline
	SOL & 0,116503 \\
	\hline
	HASH\_0 & 0,018602 \\
	\hline
	HASH\_1 & 0,011834 \\
	\hline
	HASH\_2 & 0,018971 \\
	\hline
	HASH\_3 & 0,029064 \\
	\hline
	HASH\_4 & 0,024272 \\
	\hline
	HASH\_5 & 0,017449 \\
	\hline
	MONTH & 0,076722 \\
    \hline  
    \end{tabular}
    \label{tab:addlabel}\\
  {\scriptsize Fuente: Elaboración Propia}
\end{table}

Se evidencia que la variable \textit{TEMP\_MED, SOL, PREC, PRES\_MIN} toman mayor importancia a nuestro modelo, es decir a portan mayor información para predecir la tasa de incidencia de covid-19, las variables HASH no toman mayor importancia ya que el conjunto de estas seis toman la representación de cada uno de los valores que tenía la variable \textit{PROVINCIA} anteriormente, por otro lado, las provincias son las encargadas de segmentar el dataset ya que cada una tiene sus propias condiciones climáticas.
\\
\item \textbf{SERIE TEMPORAL FORECASTERAUTOREG}
\\\\
Luego de entrenar y ajustar nuestro modelo se realiza la comparación de los valores reales con los predichos por nuestro modelo ForecasterAutoreg, notamos que las predicciones están más alejadas con respecto a la real, es decir, esta prediciendo una tasa de incidencia más alta para el periodo 2022-03 en adelante, pero tiene una pequeña similitud con respecto a la tendencia de la gráfica. Las razones de las diferencias son las mismas expuestas en los modelos anteriores.
\newpage
\begin{figure}[H]
	\begin{center}
	\includegraphics[width=0.86\textwidth]{real_vs_pred_serie_temp}
		\textbf{\caption{{\small Valor predicho vs Valor real Modelo ForecasterAutoreg. \label{tb:real_vs_pred_serie_temp}}}}
	{\scriptsize Fuente: [Elaboración propia]}
	\end{center}
\end{figure}
Las métricas calculadas para la evaluación del modelo son:
\begin{itemize}
\item \textbf{$R^2 = -6.46$}
\item \textbf{$MAE = 37937.62$}
\item \textbf{$RMSE = 42756.51$}
\end{itemize}

El modelo tiene un $R^2$ negativo de -6,46 es decir nuestro modelo no es capaz de explicar la variabilidad observada de la tasa de incidencia. Las predicciones del modelo se alejan en promedio 37937,62 unidades de nuestro valor real esto es explicado por el MAE ya que calcula el error en la misma escala de los datos. Por el contrario, el valor RMSE de 42756,51.
\\\\
Mediante el método \textbf{get\_feature\_importances()} nos devuelve la importancia de las características basadas en impurezas del modelo almacenado en el pronosticador. Sólo es válido cuando el pronosticador ha sido entrenado usando como regresor GradientBoostingRegressor o RandomForestRegressor, para nuestro caso este último.

\begin{table}[H]
  \centering
   \textbf{\caption{{\small Importancia de las variables ForecasterAutoreg. \label{tb:TableImportForecas}}}}       
	\renewcommand{\arraystretch}{1.2}  
  %{\setlength{\arrayrulewidth}{0.3mm}
    \begin{tabular}{|c|c|}
    \hline
    \rowcolor{Peach} Variable & Importancia \\
	\hline        
    lag\_1 & 0,097890 \\
	\hline
    lag\_2 & 0,092794 \\
    \hline
    lag\_3 & 0,377607 \\
    \hline
    lag\_4 & 0,069648 \\
    \hline
    lag\_5 & 0,047422 \\
	\hline
	lag\_6 & 0,113457 \\
	\hline
	lag\_7 & 0,077389 \\
	\hline
	lag\_8 & 0,059880 \\
	\hline
	lag\_9 & 0,031993 \\
	\hline
	lag\_10 & 0,031921 \\
	\hline
    \end{tabular}
    \label{tab:addlabel}\\
  {\scriptsize Fuente: Elaboración Propia}
\end{table}

Se evidencian en la tabla 8 que los 10 lags que se utilizaron para entrenar el modelo en donde para cada lag devolvió la importancia de todas las características, para el lag\_3 tomo más importancia las características o variables para explicar nuestra variable dependiente, seguido por el lag\_6, lag\_1 y lag\_2 respectivamente; los lags que tomaron menor importancia fueron los lag\_9 y lag\_10.
\\
\item \textbf{K-MEANS}
\\\\
Luego de entrenar y ajustar nuestro modelo y de asignar el clúster correspondiente a cada data, se agrupa el datataset por medio del clúster y las demás variables como medida se tomo la media, el resultado fue el siguiente:
\begin{figure}[H]
	\begin{center}
	\includegraphics[width=0.9\textwidth]{cluster_df_kmeans}
		\textbf{\caption{{\small Características climatológicas por clúster. \label{tb:cluster_df_kmeans}}}}
	{\scriptsize Fuente: [Elaboración propia]}
	\end{center}
\end{figure}
Se evidencia que el clúster 0 tiene una tasa de incidencia más baja, seguido por el clúster 2 y el clúster 1 que tiene una tasa de incidencia más alta. El clúster 0 tiene una temperatura media superior, seguido por el clúster 2 y con una temperatura más baja el clúster 3; de igual forma en el mismo orden tiene más horas de sol y una racha mayor el clúster 0; la altitud juega un papel importante en la determinación de factores climáticos de una región, ya que a menor altura es más probable que tenga una temperatura mayor, es por esta razón que el clúster 1 a pesar de tener un poco más horas de sol con respecto al clúster 2, tiene una temperatura menor, ya que está a una altitud mayor que el clúster 2. Estas son las variables que toman más importancia, por experiencia en los modelos anteriores y se evidencia un patrón claro en cada clúster creado. 
\\\\
Para evidenciar este fenómeno gráficamente, se realiza el análisis PCA para transformar nuestras variables y dejar dos componentes $(X, Y)$, estos dos componentes se transforman en un dataframe y se les asignan unas nuevas columnas para tener más información al momento de graficar $X$ y $Y$. Las columnas agregadas a nuestro PCA fueron el clúster correspondiente la temperatura promedio, la tasa de incidencia y un color correspondiente a cada clúster, esto para evidenciar mejor el fenómeno de segmentación. 
\\\\
La representación gráfica del PCA con los clúster se muestra a continuación, en donde se evidencia claramente la segmentación y separación de los tres clústers definidos, en donde nuestro clúster 0 (azul) tiene unas tasas de incidencia menor; el clúster 1 (amarillo) tiene unas tasas de incidencias más altas y el clúster 2 (verde) tiene unas tasas de incidencia intermedias, las características climatológicas de cada clúster se explicaron en la figura \textbf{\ref{tb:cluster_df_kmeans}}.
\newpage

\begin{figure}[H]
	\begin{center}
	\includegraphics[width=1\textwidth]{pca_kmeans}
		\textbf{\caption{{\small PCA K-means. \label{tb:pca_kmeans}}}}
	{\scriptsize Fuente: [Elaboración propia]}
	\end{center}
\end{figure}

En la figura anterior se muestra en total 1976 puntos, que equivalen a los 38 meses de cada uno de las 52 provincias. Se realizó el estudio agrupando estos datos por clúster y por provincia, llegando a una cantidad de 144 registros, y aquí el fenómeno claro e interesante es que por lo general casi todas las provincias hacen parte de los tres clúster ya que sus condiciones climáticas varían a lo largo del año, pero por regla general cuando cada provincia perteneció al clúster 0 tenía menor tasa de incidencia, por el contrario, cuando estaba en el clúster 1 tenía mayor tasa de incidencia.	
\end{itemize}

\subsubsection{CONTRASTE ENTRE MODELOS}

\newpage
\begin{center}
\section{CONCLUSIÓN Y TRABAJOS FUTUROS}
\end{center}
XXXXXX

\newpage
%\begin{center}
%\section{REFERENCIAS}
%\end{center}
\clearpage
\renewcommand{\refname}{}
\begin{thebibliography}{90}
\vspace{-\baselineskip}
\vspace{-\baselineskip}
\section{REFERENCIAS}
%\addcontentsline{toc}{section}{REFERENCIAS}

\bibitem[1]{constrainedClimate}

Araujo, M. B., \& Naimi, B. (2020). Spread of SARS-CoV-2 Coronavirus likely to be constrained by climate. MedRxiv, 2020.03.12.20034728. https://doi.org/10.1101/2020.03.12.20034728

\bibitem[2]{HighTemperatue}
Wang, J., Tang, K., Feng, K., \& Lv, W. (2020). High Temperature and High Humidity Reduce the Transmission of COVID-19. Available at SSRN 3551767. https://ssrn.com/abstract=3551767

\bibitem[3]{MachineLearning}
Mariette Award \& Rahul Khanna. (2015). Efficient Learning Machines, Theories, Concepts, and Applications for Engineers and System Hesigners. https://link.springer.com/book/10.1007/978-1-4302-5990-9

\bibitem[4]{RevistaTIA}
Ingry Nathaly Salamanca Rativa \& Edgar Junior Castro Escorcia. (2019). Técnicas de aprendizaje automático aplicadas en los sistemas de predicción. https://revistas.udistrital.edu.co/index.php/tia/article/download/17325/17214/104552

\bibitem[5]{Uma}
Valenzuela González, Gema. (2022). Aprendizaje Supervisado: Métodos, Propiedades y Aplicaciones. https://riuma.uma.es/xmlui/handle/10630/25147

\bibitem[6]{Clustering}
Jesús Bobadilla Sancho. (2020). Machine Learning y Deep Learning Usando Python, Scikit y Keras. https://www.perlego.com/book/2165268/machine-learning-y-deep-learning-pdf

\bibitem[7]{PCA}
Raúl Benítez, Andrés Cencerrado Barraqué, Gerard Escudero \& Samir Kanaan. (2020). https://openaccess.uoc.edu/bitstream/10609/140427/8/Inteligencia%20artificial%20avanzada_M%C3%B3dulo%201_Inteligencia%20artificial%20avanzada.pdf

\bibitem[8]{CNN}
Aston Zhang, Zachary Lipton, Mu Li \& Alexander J. Smola. (2021). Dive into Deep Learning, Convolutional Neural Networks. https://www.d2l.ai/chapter\_convolutional-neural-networks/index.html

\bibitem[9]{RNN}
Simeon Kostadinov. (2020). Recurrent Neural Networks With Python Quick Start Guide: Sequential Learning and Lenguage modeling
%URL: $https://www.buscalibre.com.co/libro-recurrent-neural-networks-with-python-quick-start-guide-sequential-learning-and-language-modeling-with-tensorflow-libro-en-ingles/9781789132335/p/54514893$

%\bibitem[10]{nvidia}
%Edgar Zepeda Urzua. (2021). Aprendizaje Federado con FLARE: NVIDIA Lleva la Inteligencia Artificial Colaborativa a el Área de la Salud y más allá. https://hardwareviews.com/aprendizaje-federado-con-flare-nvidia-lleva-la-inteligencia-artificial-colaborativa-a-el-area-de-la-salud-y-mas-alla/

\bibitem[10]{regresion lineal}
Aurélien Vannieuwenhuyze. (2020). Inteligencia Artificial Fácil - Machine Learning y Deep Learning Prácticos. https://www.buscalibre.com.co/libro-inteligencia-artificial-facil-machine-learning-y-deep-learning-practicos-libro-en-castilian-aurelien-vannieuwenhuyze-eni/9782409025327/p/52851824

\bibitem[11]{regresion lineal 2}
Joaquín Amat Rodrigo. (2016). Introducción a la Regresión Lineal Múltiple.
URL: $https://www.cienciadedatos.net/documentos/25_regresion_lineal_multiple$

%\bibitem[12]{DALL-E}
%Alex Hughes. (2022). Dall-E 2: Why the AI image generator is a revolutionary invention
%URL: $https://www.sciencefocus.com/future-technology/dall-e-2-why-the-ai-image-generator-is-a-revolutionary-invention/$

%\bibitem[13]{StyleGAN2}
%SYNCED. (2020). MIT CSAIL Uses Deep Generative Model StyleGAN2 to Deliver SOTA Image Reconstruction Results
%URL: $https://syncedreview.com/2020/12/10/mit-csail-uses-deep-generative-model-stylegan2-to-deliver-sota-image-reconstruction-results/$

\bibitem[12]{random forest}
Sruthi E R. (2023). Understand Random Forest Algorithms With Examples (Updated 2023).
URL: $https://www.analyticsvidhya.com/blog/2021/06/understanding-random-forest/$

\bibitem[13]{random forest 2}
VLADIMIR LYASHENKO. (2020). How to use random forest for regression: notebook, examples and documentation.
URL: $https://cnvrg.io/random-forest-regression/$

\bibitem[14]{ModeloGeometrico}
Arnaldo Torres-Degró. (2011). Tasas de crecimiento poblacional (r): Una mirada desde el modelo matemático lineal, geométrico y exponencial. https://revistas.upr.edu/index.php/cidedigital/article/download/11774/9736/11342

\bibitem[15]{skcilearn}
scikit-learn. (2023). scikit-learn Machine Learning in Python. https://scikit-learn.org/stable/index.html

\bibitem[16]{series temporales}
Santiago de la Fuente Fernández. (2020). series temporales. https://www.estadistica.net/PAU2/series-temporales.pdf

\bibitem[17]{series temporales 2}
José Alberto Mauricio. (2005). Introducción al análisis de series temporales. https://www.ucm.es/data/cont/docs/518-2013-11-11-JAM-IAST-Libro.pdf

\bibitem[18]{forescasting}
Joaquín Amat Rodrigo, Javier Escobar Ortiz. (2023). Skforecast: forecasting series temporales con Python y Scikit-learn. https://www.cienciadedatos.net/documentos/py27-forecasting-series-temporales-python-scikitlearn.html

\bibitem[19]{series temp 3}
Francisco Parra. (2019). Estadística y Machine Learning con R. https://bookdown.org/content/2274/series-temporales.html

\bibitem[20]{series temp estado arte}
Khasawneh, A. I., Humeidan, A. A., Alsulaiman, J. W., \& Bloukh, S. H. (2020). Modeling and prediction COVID-19 in Jordan under different intervention scenarios. Informatics in Medicine Unlocked, 19, 100345.

\bibitem[21]{LSTM est arte}
Chimmula, V. K. R., \& Zhang, L. (2020). Time series forecasting of COVID-19 transmission in Canada using LSTM networks. Chaos, Solitons \& Fractals, 135, 109864.

\bibitem[22]{redes est arte}
Chen, S., Yang, J., Yang, W., Wang, C., \& Bärnighausen, T. (2020). COVID-19 control in China during mass population movements at New Year. The Lancet, 395(10226), 764-766.

\bibitem[23]{aprend auto est arte}
Yoo, S., Lee, S., Lee, D., Lee, C. K., \& Kim, Y. K. (2020). Effective prediction of COVID-19 using deep learning models: A case study of South Korea. Chaos, Solitons \& Fractals, 140, 110153.

\end{thebibliography}
\newpage
%\begin{center}
\section*{APÉNDICE I}
\addcontentsline{toc}{section}{\protect\numberline{}APÉNDICE I}
%\end{center}



\newpage
\begin{center}
\section*{ANEXOS I}
\addcontentsline{toc}{section}{\protect\numberline{}ANEXOS I}
\end{center}
\begin{table}[H]
  \centering
   \textbf{\caption{{\small Variables y descripción del set de datos de COVID-19 por provincias. \label{tb:testTable}}}}
       
	\renewcommand{\arraystretch}{1.2}  
  %{\setlength{\arrayrulewidth}{0.3mm}
    \begin{tabular}{|c|p{11cm}|}
    \hline
    \rowcolor{Peach} Variable & Descripción \\
	\hline    
    provincia\_iso & \href{https://es.wikipedia.org/wiki/ISO_3166-2:ES}{Código ISO} de la provincia de residencia. NC (no consta) \\
    \hline
    sexo & Sexo de los casos: H (hombre), M (mujer), NC (no consta) \\
    \hline
    \multirow{2.5}{*}{grupo\_edad} & Grupo de edad al que pertenece el caso: 0-9, 10-19, 20-29, 30-39, 40-49, 50-59, 60-69, 70-79, $\geq$ 80 años. NC: no consta. Después del 28 de Marzo solo grupos de más de 60 años. \\
    \hline
    \multirow{9}{*}{fecha} & \textbf{Casos:} En los casos anteriores al 11 de mayo, se utiliza la fecha de diagnóstico, en su ausencia la fecha de declaración a la comunidad y, en su ausencia, la fecha clave (fecha usada para estadística por las CCAA). En los casos posteriores al 10 de mayo, en ausencia de fecha de diagnóstico se utiliza la fecha clave1. \textbf{Hospitalizaciones, ingresos en UCI, defunciones:} los casos hospitalizados están representados por fecha de hospitalización (en su defecto, la fecha de diagnóstico, y en su defecto la fecha clave, los casos UCI por fecha de admisión en UCI  (en su defecto, la fecha de diagnóstico, y en su defecto la fecha clave) y las defunciones por fecha de defunción  (en su defecto, la fecha de diagnóstico, y en su defecto la fecha clave).  \\
    \hline
    \multirow{6}{*}{num\_casos} & Número de casos notificados confirmados con una prueba diagnóstica positiva de infección activa (PDIA) tal como se establece en la Estrategia de detección precoz, vigilancia y control de COVID-19 y además los casos notificados antes del 11 de mayo que requirieron hospitalización, ingreso en UCI o fallecieron con diagnóstico clínico de COVID19, de acuerdo a las definiciones de caso vigentes en cada momento. \\
	\hline
	num\_hosp & Número de casos hospitalizados  \\
	\hline
	num\_uci & Número de casos ingresados en UCI \\
	\hline
	num\_def & Número de defunciones \\
	\hline
    \end{tabular}
  %\label{tab:addlabel}\\
  {\scriptsize Fuente: \href{https://cnecovid.isciii.es/covid19/resources/metadata_casos_hosp_uci_def_sexo_edad_provres.pdf}{RNVD}}
\end{table}







\end{document} 

\ifpdf
    \graphicspath{{1_introduction/figures/PNG/}{1_introduction/figures/PDF/}{1_introduction/figures/}}
\else
    \graphicspath{{1_introduction/figures/EPS/}{1_introduction/figures/}}
\fi
